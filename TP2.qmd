---
title: "Labs2 - Analyse en composante Principale et au-delà"
author:
  - name: Marie-Pierre Etienne
    affiliation:
      - ENSAI - CREST
    email: marie-pierre.etienne@ensai.fr
date: "Last updated on `r format(Sys.time(), '%d %B, %Y')`"
institute: https://marieetienne.github.io/MAF/
bibliography: TPs.bib
execute:
  freeze: auto
editor:
  markdown:
    wrap: 72
css: mpe_pres_revealjs.css
---

```{r}
#| label: setup_hid
#| include: false
#| eval: true

library(tidyverse)
library(wesanderson)
knitr::opts_chunk$set(echo = TRUE, comment = NA, cache = TRUE, message = FALSE,
                      warning = FALSE, eval = TRUE,
                      fig.align = "center")
theme_set(theme_minimal())
options(ggplot2.discrete.colour=   scale_color_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) )
couleur <-  wesanderson::wes_palette(name = "Darjeeling1")
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / ( sqrt((length(x)-1) / length(x)) *sd(x, na.rm) )
```

::: hidden
\$\$

\newcommand\R{{\mathbb{R}}}
\newcommand\Xbf{{\boldsymbol{X}}}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\xcol[1]{\boldsymbol{x}^{#1}}
\newcommand\xrow[1]{\boldsymbol{x}_{#1}}
\newcommand\xbf{\boldsymbol{x}}
\newcommand\ybf{\boldsymbol{y}}

\$\$
:::

# Statistiques avec R à l'Ensai

Le Logiciel R est libre et disponible sur divers plateformes [sur cette page](https://www.r-project.org/) et le logiciel [Rstudio](https://posit.co/download/rstudio-desktop/) est une interface agréable pour l'utilisation de R  Vous pouvez installer ces logiciels sur votre ordinateur, toutefois il faut être prêt à gérer soi-même les problèmes de version, d'installation etc.

Pour plus de sérénité il est préférable d'utiliser les serveurs auqxuels vous pouvez vous connecter par l'url suivante (depuis n'importe où)

[http://clust-n1.ensai.fr \; ou \; http://clust-n2.ensai.fr]{.center}

Pour accéder en plus à une interface graphique, par exemple pour utiliser `shiny` et notamment `Factoshiny`, il faut être à l'ENSAI et utiliser l'adresse


[http://clust-n1.domensai.ecole \; ou \; http://clust-n2.domensai.ecole]{.center}

# De bonnes pratiques

## Préparation de l'environnement

* Ouvrir un navigateur web et se connecter au serveur de calcul.
* Créer un Projet  (File --> New Project) ou bien en cliquant en haut à droite.  Le travail par projet facilite l'accès aux données (notamment lorsque l'on passe d'un ordinateur à l'autre) et permet surtout de "zapper" d'un projet à un autre.
* Choisissez Créer un projet depuis un Nouveau Repertoire, nommez le `MAF_TD2` (comme toujours en programmation on évite les caractères spéciaux, c'est-à-dire, les accents, les espaces entre autres)
* Pour pouvoir garder le code et l'interprétation des résultats au même endroit nous allons utiliser un type de document [Quarto](https://quarto.org/) qui est adapté pour mélanger code et texte. Quarto accepte du code R mais aussi Python et Julia. Créer un fichier `TP2.qmd`. Vous pouvez utiliser [le fichier d'exemple](tp_maf_exemple.qmd).
* Passez en mode `source`(en haut à gauche) au lieu de `Visual` ceci permet de mieux comprendre la structure d'un document quarto.

## Gestion de packages

Nous utilisons la suite de packages `tidyverse` pour la manipulation de données, `FactomineR` pour la mise en oeuvre des analyses factorielles et éventiuellement `factoextra` pour des sorties plus jolies. (pour une exploration plus interactive on pourra aussi utiliser `Factoshiny` ou `explor`).


::: callout-tip
 Une bonne pratique consiste à ne charger que le nombre minimal de packages nécessaires. Evitez de commencer tous les codes avec la même liste de package, pesez l'intérêt de chacun d'eux.
:::



## Structure Document Quarto

Un document Quarto se divise en plusieurs sections clés qui facilitent l'organisation de l'analyse, l'intégration de code, et la présentation des résultats. Voici une présentation des principales composantes :

### En-tête YAML

C'est la section de métadonnées du document. Elle est située en haut du fichier et est encadrée par des lignes ---. On y définit des informations essentielles telles que :
        title : le titre du document ;
        author : l’auteur ou les auteurs du document ;
        date : la date de création, qui peut être dynamique avec R, par exemple en utilisant date: "r Sys.Date()" ;
        format : le format de sortie (HTML, PDF, Word, revealjs pour les présentations, etc.).

    Exemple d'en-tête YAML :

```{yaml}
#| echo: true

---
title: "Analyse de Données avec R"
author: "Votre Nom"
date: "`r Sys.Date()`"
format: html
---

```

### Texte en Markdown

La rédaction du texte dans Quarto utilise la syntaxe Markdown pour structurer le contenu de manière simple et lisible. Markdown permet de formater le texte (titres, listes, liens, etc.) sans avoir à écrire du code complexe. Par exemple :

```{markdown}
    # pour les titres (niveau 1, 2, etc. avec ##, ###...),
    - ou * pour les listes,
    [lien](url) pour les liens.

```

Exemple :

```{markdown}
#| echo: true

## Introduction
Ce document présente une analyse de données en utilisant **R**.
- Point 1
- Point 2

```

Le markup langage markdown permet aussi d'insérer des formules mathématiques, comme du latex.

```{markdown}
  $$x_{+k}= \sum_{i=1^n} x_{ik}$$

```

### Blocs de Code
Quarto permet d'intégrer du code R (mais aussi  Python, et Julia) grâce à des blocs de code, `chunks`. Les blocs de code sont délimités par trois accents graves (triple backtick), avec le langage spécifié entre accolades ({}). Les blocs peuvent exécuter du code directement dans le document et afficher les résultats en ligne.

Exemple de bloc de code en R :

```{r}
#| label: chunk_example
#| echo: fenced
#| eval: false

 # Calcul simple
  2+2
```


Les options de rendu du bloc de code peuvent être précisées. Par exemple ici

* #| label: identifie le morceau de code, pratique pour identifier les erreurs

* #| eval: false permet de ne pas executer le code

Une présentation plus complète des options, pour un rendu plus fin est disponible sur [le site de Quarto](https://quarto.org/docs/computations/r.html#chunk-options).

Le bloc peut être défini en tapant

- manuellement les triples backticks,
- ou en utilisant le menu Code --> Insert Chunk,
- ou avec le raccourci CTRL + ALT + i


### Rendu et exportation du document

Une fois le document terminé, on peut le rendre dans le format souhaité en utilisant la commande suivante dans le terminal `quarto render mon_document.qmd`ou en cliquant sur Render dans RStudio.

Cette commande crée une version finalisée du document dans le format spécifié (HTML, PDF, etc.).


Avec Quarto on peut tout autant faire des slides, des rapports en pdf, en word etc. Ceci permet de s'assurer que les résulats présentés sont ceux de l'analyse et c'est un *premier pas important dans le sens d'assurer plus de reproductibilité en science*. (d'autres outils sont utiles voir [ici](https://marieetienne.github.io/reproductibilite/_presentation/#1) )

C'est très utile en TP puisque vous pouvez commenter les analyses proposées, les résultats obtenus directement dans le fichier qui contient les codes permettant d'obetenir ces résultats.


Plus largement, en suivant cette structure, vous pouvez organiser vos analyses de manière professionnelle et claire.

### Création du document

Lors de la mise en place de l'analyse, on peut éxécuter chaque chunk avec le raccourci CTL + Enter, ou en cliquant sur la petite flèxche verte en haut à droite du chunk.



# Le jour du dépassement

Les méthodes d'analyse factorielles sont utiles pour l'exploration de
données, soit en tant que telle soit dans une phase préparatoire à un
travail de modélisation. Il est utile de disposer à la fois d'outils
interractifs pour l'exploration, mais également de garder une trace de
l'exploration avec un document bilan.

Nous allons dans ce TP utiliser un fichier `TP2.qmd` pour garder une
trace des analyses faites mais nous pourrons en parallèle utiliser le
potentiel d'une exploration plus interactive soit avec `Factoshiny` soit
avec `explor` selon ce que vous préférez.

Dans ce TP nous allons explorer un premier exemple en détail avec une
démarche bien guidée tandis que le second exemple a pour objectif de
vous laisser face aux données en autonomie, pour vérifier que vous voyez
comment aborder un jeu de données avec les méthodes d'analyse
factorielle.

## Description des données

Le jeu de données utilisé dans ce premier cas pratique provient du [site
web de Global Footprint
Network](https://www.footprintnetwork.org/licenses/public-data-package-free/).
Il contient les résultats d'empreinte écologique et de biocapacité pour
184 pays.

Les données sont [disponibles sur ce
lien](https://marieetienne.github.io/datasets/overshootday_overview.csv).

### Quelques définitions

Le calcul de l'empreinte écologique et de la biocapacité nous aide à
répondre à la question de recherche fondamentale : Quelle est la demande
des êtres humains envers les surfaces biologiquement productives
(empreinte écologique) par rapport à la quantité que la planète (ou la
surface productive d'une région) peut régénérer sur ces surfaces
(biocapacité) ?

-   Hectare global (gha) : C'est l'unité choisie pour exprimer toutes
    les quantités d'intérêt concernant la consommation/émission de
    carbone. Une unité de surface correspondant à la productivité
    moyenne d'un hectare de terres mondiales. Un hectare de terres
    agricoles vaudra plus d'hectares globaux qu'un hectare de désert.
-   Empreinte écologique (en gha par personne) : Le nombre de gha requis
    pour produire les besoins et absorber les déchets d'un pays.
-   Biocapacité (en gha) : La capacité d'un pays à produire ce dont il a
    besoin et à absorber ses déchets (réserve écologique).
-   Jour de dépassement : Jour de l'année où la demande d'un pays
    dépasse sa biocapacité annuelle.


L'unité utilisé pour comparer est le Global Hectare (gha): Unité de
surface qui correspondrait à la production moyenne d'un hectare du
monde. Un hectare de champ vaudra donc plus de global hectare qu'un
hectare de desert.

Le gha par personne correspond à une production (ou consommation)
globale d'une unité, (ici un pays) divisé par le nombre d'habitants dans
cette unité.

Les **Variables disponibles** sont

1.  **life_expectancy** : Espérance de vie moyenne (années).
2.  **hdi** : Indice de développement humain.
3.  **per_capita_gdp** : PIB par habitant (USD).
4.  **region** : Région géographique.
5.  **income_group** : Catégorie de revenu (HI : Haut, UM : Moyen
    supérieur, LM : Moyen inférieur, LI : Faible).
6.  **pop** : Population (en millions).
7.  **total_prod** : Production totale (gha par personne) la production
    du pays par habitant.
8.  **total_cons** : Consommation totale (gha par personne) la
    consommation par habitant.
9.  **biocapacity** : La production durable (gha par personne) par
    habitant, ce que peut fournir le pays ramené par habitant.
10. **number_of_countries_required** : Nombre de pays nécessaires pour
    satisfaire la consommation (total_cons/biocapacity)
11. **number_of_earths_required** : Nombre de terres dont on aurait
    besoin si tout le monde faisait comme dans le pays en question (nb
    gha par personnes du pays / 1.583 (nb de gha par habitant de la
    terre, ce chiffre est mis à jour chaque année))
12. **overshoot_day** : Jour de dépassement (numéro du jour dans
    l'année).


Des détails supplémentaires sont disponibles
[ici](https://data.footprintnetwork.org/?_ga=2.237587203.280109455.1689844989-712229654.1682588383#/abouttheData)

## Objectifs

Les relations entre les différentes mesures de l'empreinte écologique et
les caractéristiques des pays.

### Chargement des Données

- Charger les données

```{r}
#| label: load_data
#| echo: false
#| eval: true
#| output: false
# Chargement des données
overshoot_dta <- read.table("https://marieetienne.github.io/datasets/overshootday_overview.csv",
                            sep = ",",
                            header = TRUE,
                            row.names = 1)

head(overshoot_dta)
```

### Analyse Exploratoire des Données

- Pouvez vous indiquer le nombre de lignes et de colonnes ?

- Proposez des résumés numériques pour chaque variable, ainsi qu'une étude
des corrélations.

```{r}
#| label: desc_data
#| echo: false
#| include: false

overshoot_dta |>
  mutate(across(c(region, income_group), ~as.factor(.x))) |>
  summary()
```

Pour le moment, on va supprimer les données manquantes (on verra par la
suite comment gérer ce problème de manière plus satisfaisante). Pour
ceci on peut utiliser `drop_na` du package tidyverse ou `na.omit` de R
Base.

```{r}
#| label: drop_na
#| echo: false
#| include: false

overshoot_dta <- overshoot_dta |> drop_na()
summary(overshoot_dta)
```

## Construire une ACP

-   Quel poids choisir pour les pays ?

-   Quelle métrique choisir ?

-   Quelles variables met-on en variables supplémentaires ?

```{r}
#| label: PCA
#| include: false
library(FactoMineR)
library(explor)
library(factoextra)
#avec la population comme poids
overshoot_pca <- PCA(overshoot_dta,
                     scale.unit = TRUE,
                     quali.sup = c(4,5),
                     row.w = overshoot_dta[,6],
                     quanti.sup = c(6, 11))

#meme poids pour tous les pays, la population devient une variable comme les autres. on verra qu'elle est assez orthogonale au premier plan
overshoot_pca <- PCA(overshoot_dta,
                     scale.unit = TRUE,
                     quali.sup = c(4,5),
                     quanti.sup = c(11))

#explor(overshoot_pca)




plot(overshoot_pca, axes = c(1,2), choix = "var")

fviz_pca_ind(overshoot_pca,  col.ind = "cos2")

plot(overshoot_pca, axes = c(1,2), choix = "ind", habillage = 5)

plot(overshoot_pca, axes = c(1,2), choix = "ind", habillage = 5) + ylim(c(-10,10))
```

-   Quelle est l'inertie portée par l'axe 1 , l'axe 2, l'axe 3 ?

```{r}
#| label: inertia
#| include: false
overshoot_pca$eig[1:3,1]
```

-   Commentez la qualité de la représentation sur le plan (1-2), puis
    (1-3).
    <!-- Le plan 1-2 représente  `r overshoot_pca$eig[2,3]` d'inertie, tandis que le plan 1-3 en représente `r sum(overshoot_pca$eig[c(1,3),2])`. Le premier axe porte à lui seul `r overshoot_pca$eig[1,3]` d'inertie. -->
    <!-- lorsque l'on fait une acp en prenant la pop en compte et sans mettre les poids, on voit que l'axe 3 est la population, cela signifie que la population d'un pâys est indépendante des 2 premiers axes dans la suite on pourrait déduire qu'il y a de gros pays avec une grosse empreinte carbone mais aussi des gros pays avec une faible empreinte carbone  -->

-   De quelles variables nous parle l'axe 1 ? l'axe 2 ? Pour répondre à
    cette question on regardera le cercle des corrélations mais aussi
    les contributions des variables à la création des axes.


-   Que pensez vous du graphe des individus ? Quels sont les individus
    atypiques ? Quels sont les pays les mieux représentés ?

-   Comment l'ACP est-elle modifiée si on retire Singapour de l'analyse ?



- De nombreuses variables nous parlent de l'empreinte écologique, et les
variables nous parlant du développement du pays sont moins nombreuses.
On peut rééquilibrer l'importance des variables avec une méthodes vue en
cours, laquelle ?

On souhaite distinguer deux groupes de variables : les variables
    concernant l'empreinte carbone et les variables portant sur le
    développement du pays.

-   Faites 2 ACP différentes sur chacun de ces deux groupes. Quelle est
    la première valeur propre dans les deux cas ?

-   Rappelez en quoi la méthode mentionnée plus haut est une ACP
    particulière.

-   On réaliser une AFM grâce la fonction `̀MFA` de FactoMineR.
    Reprenez l'analyse précédente avec ce nouvel équilibre entre les
    différentes variables équilibrées. Quelles sont les informations que
    vous pouvez retenir ?

Attention on ne peut pas indiquer de variables qualitatives
supplémentaires dans MFA

```{r}
overshoot_quanti <-  overshoot_dta |>
  select(where(is.numeric))
```

```{r}

overshoot_mfa <- MFA(overshoot_quanti,
    group = c(4, 6), ## 4 variables dans le premier groupe, les 6 suivantes dans le groupe 2
    type = rep("s", 2) ## le groupe est de type quanti qu'on veut normaliser par l'eacrt type, meme chose pour le groupe 2
)
```
- En vous servant de l'aide, indiquez ce que signie ce que l'argument `type = rep("s",2)` signifie.

- La visualisation est elle grandement modifiée, est-ce surprenant ?


# ACP pour compresser l'information : analyse d'images.

Cet exercice propose d'utiliser l'ACP pour compresser une image.

-   Connaissez-vous cette photo ?

![](img/abbey_road.jpg)

## Objectif

Une image en noir et blanc peut être envisagée comme une matrice où
chaque contient le niveau de gris du pixel qu'il représente. On stocke
donc autant de valeurs que de pixel. Peut on compresser l'information en utilisant l'ACP ?

## Préparation

-   Téléchargez cette image en suivant {le lien
    suivant](https://marieetienne.github.io/MAF/img/abbey_road.jpg) et
    placez la dans votre répertoire de travail dans
    `img/abbey_road.jpg`.

Nous allons utiliser les packages suivants:
Nous allons utiliser les packages

```{r}
#| label: setup
#| echo: true
#| eval: true

library(imager) ## pour manipuler les images
library(parallel) # pour les calculs sur les images
```

Par ailleurs pour manipuler les images, nous aurons besoin de quelques fonctions disponibles ci-dessous

```{r}
#| label: utils
#| echo: true
#| eval: true
#| message: false
#| code-fold: true

# Function to extract blocks in parallel, handling edge cases
complete_image_with_block_size <- function(channel_matrix, block_size) {
  h <- dim(channel_matrix)[1]  # Image height
  w <- dim(channel_matrix)[2]  # Image width

  h_complete <- ifelse( h %/% block_size == h/block_size, h, (h %/% block_size +1)*block_size)
  w_complete <- ifelse( w %/% block_size == w/block_size, w, (w %/% block_size +1)*block_size)
  channel_matrix_complete <- matrix(0, ncol= w_complete, nrow = h_complete)
  channel_matrix_complete[1:h,1:w] <- channel_matrix
  return(channel_matrix_complete)
}

image2block <- function(channel_matrix, block_size) {
  h <- dim(channel_matrix)[1]  # Image height
  w <- dim(channel_matrix)[2]  # Image width
  if(  h %/% block_size != h/block_size ){
    stop("height is not a multiple of block size")
  }
  if(  w %/% block_size != w/block_size ){
    stop("width is not a multiple of block size")
  }
  # Get the number of cores available for parallel processing
  num_cores <- parallel::detectCores() - 1  # Use one less than available cores to avoid system overload

  # Define indices for parallelization
  indices <- expand.grid(seq(1, h, by = block_size), seq(1, w, by = block_size))

  # Define the function to extract a block from the matrix, handling edge effects
  extract_single_block <- function(index) {
    i <- as.numeric(index[1])
    j <- as.numeric(index[2])

    print(i)

    # Handle cases where the block exceeds the image size (edge cases)
    block <- channel_matrix[i:min(i + block_size - 1, h), j:min(j + block_size - 1, w)]

    # Flatten the block into a single row
    # If the block is smaller than 4x4 (due to edges), pad with NA or 0 to maintain consistent size
    flattened_block <- as.vector(block)

    # Pad the flattened block with NA (or 0 if you prefer) to ensure each block is of size block_size^2
    padded_block <- rep(NA, block_size * block_size)
    padded_block[1:length(flattened_block)] <- flattened_block

    return(padded_block)
  }

  # Parallelize the block extraction using mclapply
  blocks <- parallel::mclapply(1:nrow(indices), function(k) extract_single_block(indices[k,]), mc.cores = num_cores)

  # Combine the results into a matrix
  blocks_matrix <- do.call(rbind, blocks)

  return(blocks_matrix)
}

block2image <- function(image_blocks, nrow_img, ncol_img, block_size) {
  n_col_block <- ncol_img/block_size
  n_row_block <- nrow_img/block_size

  X_reconstruction_list <- lapply(1:n_col_block, function(jblock){
    list_rows <- lapply(1:n_row_block, function(iblock){
      matrix( image_blocks[(jblock-1)*n_row_block + iblock,], ncol = block_size)
    })
    do.call(rbind, list_rows)
  })
  img <-do.call(cbind, X_reconstruction_list)
  return(img)
}

```


Pour bien comprendre comment sont décomposées les images en bloc de 4 par 4, on peut créer une matrice avec des cases numérotées et constater comment elle est transformée.

On choisit des blocks de 4x4 pixels.

```{r}
#| label: block_size
#| echo: true
#| eval: true

block_size <- 4
```

Un bloc de 4x4 pixels est un individu à 16 variables.

```{r}
#| label: micro_example
#| echo: true
#| eval: false

block_size <- 4
test_image <- as.cimg(array(1:(20*16), dim= c(20, 16, 1, 1)), dim=c(32,16))
test_image[,]
plot(test_image, interpolate = FALSE )
test_image_blocks <- image2block(test_image[,], block_size = 4)
test_image_blocks
block2image(test_image_blocks, nrow(test_image), ncol(test_image), block_size = 4)

exemple2 <- matrix(10*rep(1:9, each=16), byrow= TRUE, ncol = 16)
exemple2
exemple2_img <- block2image(exemple2, nrow_img = 12, ncol=12, block_size = 4)
plot(as.cimg(exemple2_img), interpolate = FALSE )
```

On encode donc l'image des Beattles avec ce système de patches.


Pour  charger avec R, on utilise le package `imager` et la fonction `load.image`. puis la fonction `grayscale`pour bien s'assurer que l'image est codée en niveaux de gris.

```{r}
#| label: load_abbey
#| echo: true
#| eval: true

image <- load.image('img/abbey_road.jpg')
image <- grayscale(image)
dim(image)
plot(as.cimg(image), interpolate = 0, rescale = FALSE,  axes = FALSE)

## on complete l'image pour qu'elle ait une dimension multiple de la taille des blocks
abbey_comp <- complete_image_with_block_size(image, block_size = block_size)
plot(as.cimg(abbey_comp), interpolate = 0, rescale = FALSE,  axes = FALSE)
abbey_matrix <- image2block(abbey_comp, block_size = block_size)
#head(abbey_matrix)
```

-   Que représente le baycentre de ce nuage de points ?

```{r}
#| label: barycentre
#| echo: true
#| eval: true
#| message: false

barycentre <- abbey_matrix |> as_data_frame() |> summarise_all(mean) |> as.numeric()
patch_moy <- barycentre |> matrix(ncol=16, nrow=1)

block2image(patch_moy, nrow_img = 4, ncol_img = 4, block_size = 4)  |> as.cimg(width = 4, col=4)  |> plot( interpolate = 0, axes = FALSE, rescale=FALSE)
##pour voir les différences on peut rescaler les niveaux de gris
block2image(patch_moy, nrow_img = 4, ncol_img = 4, block_size = 4)  |> as.cimg(width = 4, col=4)  |> plot( interpolate = 0, axes = FALSE, rescale=TRUE)
```

### ACP sur les patches

-   Faire une ACP sur les données contenues dans `abbey_matrix` (Faut il faire une ACP normée ou non ? y a t il des variables supplémentaires)

-   Combien d'axes proposez vous de garder ?


```{r}
#| label: abbey_pca
#| echo: true
#| eval: true
#| message: false
abbey_pca <- PCA(abbey_matrix, scale.unit=FALSE, ncp=block_size*block_size, graph=FALSE)
abbey_pca$eig
```

- Que représente un vecteur propre ici ?


### Reconstruction avec 1 composante

- on souhaite reconstruire l'image à partir de l'information contenue dans une seule composante. Expliquez à quoi correspond le code ci-dessous


```{r reconstruction1}
abbey_recons_centred <- abbey_pca$ind$coord[,1,drop=FALSE] %*% t(abbey_pca$svd$V)[1,,drop=FALSE]
#on rajoute la moyenne

# X_recon_centred <- block2image(abbey_recons_centred, nrow_img = nrow(abbey_comp), ncol_img = ncol(abbey_comp), block_size = 4)
# plot(as.cimg(array(X_recon_centred, dim=dim(abbey_comp))), interpolate = FALSE, rescale = FALSE)

abbey_recons <- abbey_recons_centred |> sweep(2, barycentre, FUN = "+")
## doit etre en 0 et 1
abbey_recons_norm <- pmin(abbey_recons,1)
X_recon <- block2image(abbey_recons_norm, nrow_img = nrow(abbey_comp), ncol_img = ncol(abbey_comp), block_size = block_size)
plot(as.cimg(array(X_recon, dim=dim(abbey_comp))), interpolate = FALSE, rescale = FALSE)
```


### Reconstruction avec 2 composantes

- Modifier le code pour garder 2 composantes principales (taille de l'image divisée par 8 au lieu de 16)



### Evaluation de l'erreur de reconstruction

Une autre manière de faire serait de ne garder que la valeur moyenne pour chaque patch. On compare les deux méthodes ci-dessous.

- Ajoutez la comparaison avec la méthode qui utilise les 2 premiers composantes principales et commentez

```{r}
rmse_pca <- mean((abbey_recons-abbey_matrix )^2)
#simple filter
abbey_filter <- matrix(rowMeans(abbey_matrix), ncol = 16, nrow = nrow(abbey_matrix))
rmse_filter <- mean((abbey_filter-abbey_matrix )^2)
```



### Exercice d'extension

- On peut aussi appliquer la méthode à une image couleur en traitant séparément les
    canaux R,G,B puis recombiner.
-  Quel intérêt y aurait-il à traiter simultanément les 3 couleurs plutot que séparément ?


**Remarque:** On a représenté l'image dans un espace de plus petite dimension en appliquant un filtre sur l'image de départ.  Plutôt que de choisir un filtre standard (la moyenne de tous les pixels d'un patch), on a appris le meilleur filtre, (celui qui fait perdre le moins d'information).

En effet on a appris une transformation qui envoie une image de `r 712^2` pixels dans un espace de dimension `r round(712/4)^2`. Ce sont ces idées qui sont utilisées dans les réseaux de neurones convolutionnels à la base de l'IA pour les images.

-   Vous pouvez tester d'autres tailles de patches  (`block_size=8` ou `16`) et
    observer l'effet sur la variance expliquée et la qualité de
    reconstruction.
