---
title: "Analyse en Composantes Principales"
author:
  - name: Marie-Pierre Etienne
    affiliation:
      - ENSAI - CREST
    email: marie-pierre.etienne@ensai.fr
date: "Last updated on `r format(Sys.time(), '%d %B, %Y')`"
institute: https://marieetienne.github.io/MAF/
execute:
  freeze: auto
editor:
  markdown:
    wrap: 72
css: mpe_pres_revealjs.css
format:
  revealjs:
    theme: [default, custom.scss]
    width: 1050
    margin: 0.05
    slide-number: true
    slide-level: 2
    show-slide-number: print
    menu:
      useTextContentForMissingTitles: false
    mathjax: true  # Active MathJax
    self-contained: true
---

```{r setup, include=FALSE, eval = TRUE}
library(RefManageR)
library(tidyverse) ## to benefit from the tidyverse coding system
library(reticulate) ## to use python from R
library(wesanderson)
library(plotly)
library(ggforce)
```

```{r reference,  include=FALSE, cache=FALSE, eval = TRUE}
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "alphabetic",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("./MAF.bib", check = FALSE)
theme_set(theme_minimal())
options(ggplot2.discrete.colour=   scale_color_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) )
couleur <-  wesanderson::wes_palette(name = "Darjeeling1")
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / ( sqrt((length(x)-1) / length(x)) *sd(x, na.rm) )

```

::: hidden
\$\$

\newcommand\R{{\mathbb{R}}}
\newcommand\Xbf{{\boldsymbol{X}}}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\xcol[1]{\boldsymbol{x}^{#1}}
\newcommand\xrow[1]{\boldsymbol{x}_{#1}}
\newcommand\xbf{\boldsymbol{x}}
\newcommand\ybf{\boldsymbol{y}}

\$\$
:::

```{r datapackage, eval = TRUE, echo = FALSE, warning = FALSE}
library(plotly)
ggplot <- function(...) ggplot2::ggplot(...) + scale_fill_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) + scale_color_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) + theme_minimal()
#remotes::install_github('MarieEtienne/coursesdata', force = TRUE)
doubs.env <- read.csv ('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/DoubsEnv.csv', row.names = 1) %>% as_tibble()

data(penguins, package = 'palmerpenguins')
penguins <- penguins %>% na.omit() %>%
  mutate(year = as.factor(year))

```

# Et on commence avec les manchots

## Rappel des données sur les manchots

Données disponibles dans le package `palmerpenguins` mises à disposition
par le Dr. Kristen Gorman et the Palmer Station, Antarctica LTER.

On a mesuré les caractéristiques morphologiques de divers manchots :

::::: columns
::: {.column width="48%"}
Les 6 premières lignes (parmi 333 )

```{r extrait_penguins, echo = FALSE, eval = TRUE}

 penguins %>%
 select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%  print(n=6)
```
:::

::: {.column width="48%"}
-   bill_length_mm : la longueur du bec,

-   bill_depth_mm : l'épaisseur du bec,

-   flipper_length_mm : la longueur de la nageoire,

-   body_mass_g : le poids du corps.
:::
:::::

[Comment visualiser au mieux ces données pour faire apparaître les liens
entre variables et identifier des resemblances entre individus
?]{.rouge}

## Voir c'est comprendre : comment représenter l'information contenue dans ce tableau ?

### Objectifs

-   Représenter sans perdre trop d'information,
-   Quantifier la perte d'information,
-   Comprendre quelles sont les informations redondantes (variables
    liées),
-   Idéalement des individus éloignés dans le nuage initial, restent
    éloignés dans la représentation

Dans toute la suite, sans perte de généralité, on va considérer que les
variables sont centrées.

## Si on savait faire une Analyse en Composantes Principales

:::::: panel-tabset
### Dans R on ferait

```{r PCA_manchots_1}
#| echo: true
#| eval: true
#| output-location: column
#| results: hold
#| message: false

library(FactoMineR)
dta <- penguins %>% select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
penguins.PCA <- PCA(X = dta, ncp = 4, graph = FALSE)
```

### Représentation des individus

L'ACP permet de trouver la projection en 2 dimensions qui préserve au
mieux l'information contenue dans le nuage de points (avec peu de
déformation).

```{r PCA_manchots_ind, eval = TRUE, echo = TRUE}
#| classes: custom3565
#| output-location: column
#| results: hold
plot(penguins.PCA, choix = "ind")
```

[Reste à comprendre]{.rouge}

-   Comment quantifier l'information contenue dans un nuage de points
-   Comment est construite cette meilleure projection
-   Quels sont les individus bien représentés ?

### Représentation des variables

L'ACP définit de nouvelles variables, les composantes principales qui
sont une combinaison linéaire des anciennes variables. On peut
représenter les variables dans le premier plan de l'ACP.

```{r PCA_manchots_var, eval = TRUE, echo = TRUE}
#| classes: custom3565
#| output-location: column
#| results: hold
plot(penguins.PCA, choix = "var")
```

[Reste à comprendre]{.rouge}

-   Comment interpréter cette représentation ?
-   Quelles sont les variables bien représentées ?
-   Que peut-on déduire de ce graphique sur le lien entre les variables.
-   Comment lier graphique des individus et graphique des variables

### Visualisation par ACP

::::: columns
::: {.column width="45%"}
```{r PCA_manchots_ind_2}
#| echo: false
plot(penguins.PCA, choix = "ind")
```
:::

::: {.column width="45%"}
```{r PCA_manchots_var_2}
#| echo: false
plot(penguins.PCA, choix = "var")
```
:::
:::::
::::::

# Les données

## Mise en forme des données

L’ACP s’applique à des données quantitatives sous forme de tableau. Ce
tableau est composé de [n individus]{.alea} (lignes) et de [p
variables]{.orange} (colonnes).

Dans la suite on va noter ${\boldsymbol{X}}$ la matrice des données,
${\boldsymbol{X}}\in {\mathbb{R}}^{n\times p}$.

$x_{\class{alea}{i}\class{orange}{j}}$ est la valeur de la variable
$\class{orange}{j}$ mesurée pour l'individu $\class{alea}{i}.$

$${\boldsymbol{X}}= \overset{\color{orange}{\begin{matrix}var_1& \ldots \ &  \ \ldots \ &\  \ldots\  & \  var_p \end{matrix}}}{\begin{pmatrix}
x_{\class{alea}{1}}^{\class{orange}{1}} &  \ldots &  \ldots &\ldots &  x_{\class{alea}{1}}^{\class{orange}{p}}\\
\vdots & & &  &\vdots \\
 & &  x_{\class{alea}{i}}^{\class{orange}{j}} & & \\
\vdots & & & & \vdots \\
x_{\class{alea}{n}}^{\class{orange}{1}} & & & & \ldots x_{\class{alea}{n}}^{\class{orange}{p}}\\
 \end{pmatrix}}$$

## Deux points de vue complémentaires

::::: columns
::: {.column width="48%"}
### Le nuage des individus $C^n$

On peut considérer qu'un [individu]{.alea} $i$ est un vecteur
$\class{alea}{\boldsymbol{x}_{i}}$ dans un espace de dimension $p$. Par
convention tous les vecteurs sont des vecteurs colonnes, donc on peut
écrire

$${\boldsymbol{X}}=\begin{pmatrix}
\class{alea}{\boldsymbol{x}_{1}}^\top\\
\vdots \\
\class{alea}{\boldsymbol{x}_{n}}^\top\\
\end{pmatrix},$$

L'ensemble des $n$ vecteurs forme le [nuage des individus]{.alea} (ce
qu'on représente classiquement).

```{r nuage_ind}
#! message: false
# Installer les packages si nécessaire
# install.packages("ggplot2")
# install.packages("ggforce")

library(ggplot2)
library(ggforce)

# Créer une base de données pour les points et les annotations
individus <- data.frame(
  x = c(1.5, 0.5, -0.5, -1),   # Coordonnées des points individuels
  y = c(1.0, -0.5, 0.2, -0.5),   # Coordonnées des points individuels
  label = c("i", "", "", "")    # Nom de l'individu i
)

# Points pour O = G et l'origine des axes
origins <- data.frame(
  x = c(0.2, 1.5),
  y = c(-0.3, 0),
  label = c("O", "")
)

# Points var_k
vark <- data.frame(
  x = c(3),
  y = c(-0.3)
)



# Schéma avec ggplot
ggplot() +
  # Tracer une ellipse représentant le nuage de points
  geom_ellipse(aes(x0 = 0, y0 = 0, a = 2.5, b = 0.8, angle = 3.14159/6), fill = NA, color = "black") +

  # Ajouter les points des individus dans l'espace
  geom_point(data = individus, aes(x = x, y = y), size = 3) +

  # Ajouter les labels des individus (point i)
  geom_text(data = individus, aes(x = x, y = y, label = label), vjust = -1.5, hjust = -0.5, size = 5) +

  # Tracer les axes
  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 0), arrow = arrow(length = unit(0.3, "cm")), size = 0.5) +  # Axe des variables (horizontal)
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 2), arrow = arrow(length = unit(0.3, "cm")), size = 0.5) +  # Axe vertical
  geom_segment(aes(x = 0, y = 0, xend = -2, yend = -2), arrow = arrow(length = unit(0.3, "cm")), size = 0.5) +  # Axe vertical

  # Ajouter des lignes en pointillés (projection du point i)
  geom_segment(aes(x = 1.5, y = 1, xend = 1.5, yend = -1), linetype = "dashed") +
  geom_segment(aes(x = 1.5, y = -1, xend = 2.5, yend = 0), linetype = "dashed") +

  # Ajouter des labels mathématiques pour les distances
  # geom_text(aes(x = 2.5, y = 0.2), label = as.character(expression(x[ik] - bar(x)[k])), size = 5) +
  geom_text(aes(x = 2.5, y = 0.2), label = as.character(expression(x[i]^k)), size = 5, col = "#398d55") +

  # Ajouter des labels mathématiques pour l'espace
  geom_text(aes(x = 0.3, y = 2), label = as.character(expression(R^p)),col="#F7A913", size = 5) +

  # Ajouter l'origine O = G
  geom_text(data = origins, aes(x = x, y = y, label = label), vjust = -0.8, size = 5) +

  # Ajouter variable k
  geom_text(data = vark, aes(x = x, y = y), label = as.character(expression(var[k])), col="#F7A913", vjust = -0.8, size = 5) +

  # Ajuster le thème
  theme_minimal()  +

  coord_fixed() +  # Conserver un rapport d'aspect fixe pour les axes
  ggtitle("Représentation  du nuage des individus") +

  # Ajuster le thème
  theme (axis.title=element_blank(),
   axis.text=element_blank(),
   axis.ticks=element_blank(),
   panel.grid = element_blank())

```
:::

::: {.column width="48%"}
### Le nuage des variables $C^p$

On peut considérer qu'une [variable]{.orange} $j$ est un vecteur
$\class{orange}{\boldsymbol{x}^{j}}$ dans un espace de dimension $n$ et
on peut écrire

$${\boldsymbol{X}}=\begin{pmatrix}
\class{orange}{\boldsymbol{x}^{1}} & \ldots & \class{orange}{\boldsymbol{x}^{p}}
\end{pmatrix},$$

L'ensemble des $p$ vecteurs forme le [nuage des variables]{.orange}.

```{r nuage_var}
#! message: false
# Installer les packages si nécessaire
# install.packages("ggplot2")
# install.packages("ggforce")

# Créer une base de données pour les points et les annotations
individus <- data.frame(
  x = c(1.5, 1.2, -0.5, -1),   # Coordonnées des points individuels
  y = c(0.9, 1, 0.2, -0.5),   # Coordonnées des points individuels
  label = c("k", "j", "", "")    # Nom de l'individu i
)


# Schéma avec ggplot
ggplot() +
  # Tracer une ellipse représentant le nuage de points
  geom_ellipse(aes(x0 = 0, y0 = 0, a = 2, b = 2, angle = 3.14159/6), fill = NA, color = "black") +
  geom_ellipse(aes(x0 = 0, y0 = 0, a = 1, b = 2, angle = 3.14159/2), fill = NA, color = "black") +


  # Ajouter les labels des individus (point i)
  geom_text(data = individus, aes(x = x, y = y, label = label),  col = "#F7A913", vjust = -1.5, hjust = -0.5, size = 5) +

  # Tracer les axes
  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 0), arrow = arrow(length = unit(0.3, "cm")), size = 0.5) +  # Axe des variables (horizontal)
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 2), arrow = arrow(length = unit(0.3, "cm")), size = 0.5) +  # Axe vertical
  geom_segment(aes(x = 0, y = 0, xend = -2, yend = -2), arrow = arrow(length = unit(0.3, "cm")), size = 0.5) +  # Axe vertical

  # Ajouter des lignes en pointillés (projection du point i)
  geom_segment(aes(x = 1.5, y = 1, xend = 1.5, yend = -1), linetype = "dashed") +
  geom_segment(aes(x = 1.5, y = -1, xend = 2.5, yend = 0), linetype = "dashed") +

  # Ajouter des labels mathématiques pour les distances
  # geom_text(aes(x = 2.5, y = 0.2), label = as.character(expression(x[ik] - bar(x)[k])), size = 5) +
  geom_text(aes(x = 2.5, y = 0.2), label = as.character(expression(x[i]^k)), size = 5, col = "#F7A913") +
  geom_segment(aes(x = 0, y = 0, xend = 1.5, yend = 1), col = "#F7A913",
               arrow = arrow(length = unit(0.5, "cm")), size = 0.7)  +
  # seconde variable
  geom_segment(aes(x = 0, y = 0, xend = 1.1, yend = 1.2), col = "#F7A913",
               arrow = arrow(length = unit(0.5, "cm")), size = 0.7)  +

  # Ajouter des labels mathématiques pour l'es distances l'espace
  geom_text(aes(x = 0.3, y = 2), label = as.character(expression(R^n)),col="#398d55", size = 5) +

  # Ajouter l'origine O = G
  geom_text(data = origins, aes(x = x, y = y, label = label), vjust = -0.8, size = 5) +

  # Ajouter variable k
  geom_text(data = vark, aes(x = x, y = y), label = as.character(expression(ind[i])), col="#398d55", vjust = -0.8, size = 5) +

  # Ajuster le thème
  theme_minimal()  +

  coord_fixed() +  # Conserver un rapport d'aspect fixe pour les axes
  ggtitle("Représentation  du nuage des variables") +

  # Ajuster le thème
  theme (axis.title=element_blank(),
         axis.text=element_blank(),
         axis.ticks=element_blank(),
         panel.grid = element_blank())


```
:::
:::::

. . .

[Bien sûr les deux nuages sont intimement liés]{.question}


# Construction de la base des composantes principales

## Se donner des intuitions

```{r prep_inertie_plot}
#| echo: false

set.seed(123)
n = 50
p = 2
X = matrix(rnorm(n*p, mean = 0, sd =0.5), ncol= p) %>%
  as_tibble() %>%
  rename(x1 = V1, x2 = V2,) %>%
  mutate(x1 = x1 - mean(x1), x2 = x2 -mean(x2) ) %>%
  mutate(x3 = (x1 + 0.1 * x2)/2) %>%
  mutate(x3 = x3 -mean(x3)) %>%
  mutate(x1= x1/ (sd(x1)), x2= x2/ (sd(x2)), x3= x3/ (sd(x3)))

prov <- PCA(X[, 1:2], graph = FALSE)
X = X %>% mutate(x1_bis = prov$ind$coord[,1], x2_bis = prov$ind$coord[,2])

Xmat <- as.matrix(X)
eigen_1 <- svd(t(Xmat[,4:5])%*% Xmat[, 4:5])$v


p1 <- X %>% ggplot() + aes(x=x1_bis, y = x2_bis) + geom_point(size = 4) +# Ajuster le thème
  theme_minimal()  +
  coord_fixed() +
  geom_hline(aes(yintercept = 0), col = "gray50") +
  geom_vline(aes(xintercept = 0), col = "gray50") +

  # Ajuster le thème
  theme (axis.title=element_blank(),
         axis.text=element_blank(),
         axis.ticks=element_blank(),
         panel.grid = element_blank())

eigen_2 <- svd(t(Xmat[,c(1,3)])%*% Xmat[, c(1,3)])$v


p2 <- X %>% ggplot() + aes(x=x1, y = x3) + geom_point(size = 4) +
  coord_fixed() +
  theme_minimal()  +
  geom_hline(aes(yintercept = 0), col = "gray50") +
  geom_vline(aes(xintercept = 0), col = "gray50") +
  # Ajuster le thème
  theme (axis.title=element_blank(),
         axis.text=element_blank(),
         axis.ticks=element_blank(),
         panel.grid = element_blank())

```

::::: columns
::: {.column width="45%"}
```{r inert_p1}
p1
```
:::

::: {.column width="45%"}
```{r inert_p2}
p2
```
:::
:::::

## Se donner des intuitions

::::: columns
::: {.column width="45%"}
```{r inert_p1_var}
p1  +
  geom_segment(aes(x= 0, y = 0, xend = eigen_1[1,1], yend = eigen_1[2,1]), col = "#f76f07", arrow = arrow(length = unit(0.3, "cm"))) +
  geom_segment(aes(x= 0, y = 0, xend = eigen_1[1,2], yend = eigen_1[2,2]), col = "#d61d09", arrow = arrow(length = unit(0.3, "cm")))
```
:::

::: {.column width="45%"}
```{r inert_p2_var}
p2 +
  geom_segment(aes(x= 0, y = 0, xend = eigen_2[1,1], yend = eigen_2[2,1]), col = "#f76f07", arrow = arrow(length = unit(0.3, "cm"))) +
  geom_segment(aes(x= 0, y = 0, xend = eigen_2[1,2], yend = eigen_2[2,2]), col = "#d61d09", arrow = arrow(length = unit(0.3, "cm")))
```
:::
:::::

## Identifier $\boldsymbol{u_1}$ - Formalisation

:::::: columns
::: {.column width="45%"}
-   On cherche $\boldsymbol{u_1}$ tel que $I_{u_1}$ soit minimale mais
    $I_{u_1} + I_{u_1^\perp}=I$. [Minimiser $I_{u_1}$ revient à
    maximiser $I_{u_1^\perp}$]{.rouge}, i.e l'inertie du nuage projeté.

    -   Le premier individu se projette en
        $\class{alea}{x_{1}}^\top \boldsymbol{u_1}$ sur
        $\boldsymbol{u_1}$
    -   Le deuxième individu se projette en
        $\class{alea}{x_{2}}^\top \boldsymbol{u_1}$ sur
        $\boldsymbol{u_1}$
    -   ...

-   On rappelle que
    $${\boldsymbol{X}}= \begin{pmatrix} \class{alea}{x_{1}}^\top \\ \vdots \\ \class{alea}{x_{n}}^\top \end{pmatrix}.$$
    La projection de l'individu $i$ est obtenue en multipliant le
    vecteur ligne $\class{alea}{x_{1}}^\top$ par $\boldsymbol{u_1}$. Le
    vecteur des coordonnées sur $\boldsymbol{u_1}$ pour chaque individu
    est

$$\begin{pmatrix} \class{alea}{x_{1}}^\top \boldsymbol{u_1} \\ \vdots \\ \class{alea}{x_{n}}^\top \boldsymbol{u_1} \end{pmatrix} = X  \boldsymbol{u_1}.$$
:::

:::: {.column width="45%"}
### Maximiser $I_{u_1^\perp}$

sous la contrainte $\lVert\boldsymbol{u_1}\rVert$ \begin{align}
 I_{u_1^\perp} & = \frac{1}{n}\lVert X  \boldsymbol{u_1}\rVert^2 \\
        & = \frac{1}{n} (X  \boldsymbol{u_1})^\top (X  \boldsymbol{u_1})\\
        & =   \boldsymbol{u_1}^\top \left (\frac{1}{n} X^\top X \right)  \boldsymbol{u_1}
\end{align}

::: panel-tabset
#### Remarque

-   $\frac{1}{n} X^\top X$ est la matrice de covariance, on écrit
    souvent $\frac{1}{n} X^\top X$ sous la forme $X^\top W X$ ($W$ est
    une matrice diagonale, dont chaque terme diagonal vaut $1/n$,
    $W_{ii}$ est le poids attribué à l'observation $i$).

-   $V= X^\top W X$ est [symétrique]{.rouge} et [ positive]{.rouge}
    donc [diagonalisable]{.rouge}.

-   Il existe une matrice $P$ unitaire $P^\top P = I_p,$ et $D$ une
    matrice diagonale telle que $$V = P D P^\top.$$

-   Les termes diagonaux de $D$ sont les [valeurs propres, toutes
    positives,]{.rouge} de $X^\top W X$ rangées par odre décroissant,
    $\lambda_1 > \lambda_1 \ldots >\lambda_p$ et $U$ est la matrice des
    vecteurs propres associés.

#### Ce que ça signifie

![](change_baseXtX.png)

$\boldsymbol{b}$ est une base orthonormale, la base formée par les
vecteurs propres de $X^\top W X$.
:::
::::
::::::

## Identifier les directions principales

### Chercher $\boldsymbol{u_1}$ tel que $I_{u_1^\perp}$ soit maximale

En se plaçant dans la nouvelle base, posons
$\boldsymbol{v_1} = P^\top \boldsymbol{u_1},$
$\lVert\boldsymbol{v_1}\rVert=1$:

Maximiser
$\boldsymbol{v_1}^\top D \boldsymbol{v_1} = \sum_{k=1}^p v_{1p}^2 \lambda_p$,
sous la contrainte $\sum_{k=1}^p  v_{1p}^2=1$

$$\boldsymbol{v_1} = (1, 0, \ldots, 0)^\top, \quad \boldsymbol{u_1} = P \boldsymbol{v_1} = \begin{pmatrix} P_{11} \\ \vdots \\P_{p1} \end{pmatrix}.$$

[$\boldsymbol{u_1}$ est le premier vecteur propre]{.rouge} de
$V = X^\top W X$. (la matrice de covariance).

### Chercher $\boldsymbol{u_2}$

On cherche $\boldsymbol{u_2}\perp \boldsymbol{u_1}$, tel que
$\boldsymbol{u_2}^\top X^\top W X   \boldsymbol{u_2}$ soit maximale.

Posons $\boldsymbol{v_2} = P^\top \boldsymbol{u_2},$
$\lVert\boldsymbol{v_2}\rVert=1$: Maximiser
$\boldsymbol{v_2}^\top D \boldsymbol{v_2} = \sum_{k=1}^p v_{2p}^2 \lambda_p$,
sous la contrainte $\sum_{k=1}^p  v_{1p}^2=1$ et $v_{1p}=0$ pour assurer
l'orthogonalité avec $\boldsymbol{v_1}$

$$\boldsymbol{v_2} = (0, 1, \ldots, 0)^\top, \quad \boldsymbol{u_2} = P \boldsymbol{v_2} = \begin{pmatrix} P_{12} \\ \vdots \\U_{p2} \end{pmatrix}.$$

[$\boldsymbol{u_2}$ est le deuxième vecteur propre]{.rouge} de de
$X^\top W X$.




## Mesurer l'information préservée par projection

### Sur un axe principal

-   Lorsque l'on résume le nuage à sa projection sur un [axe
    factoriel]{.rouge} $\boldsymbol{u_k}$, on ne garde que l'information
    associée à cet axe soit $I_{\boldsymbol{u_k}^\perp}=\lambda_k$.

-   La quantité d'information préservée par projection sur l'axe $k$ est
    donc $\lambda_k$.

-   La part d'information portée par l'axe $k$ est donnée par
    $\displaystyle\frac{\lambda_k}{\sum_{k=1}^p\lambda_k}$

### Sur un axe principal

-   L'inertie du nuage projeté sur plan $k_1, k_2$ est donnée par
    $$\lambda_{k_1} +\lambda_{k_2}$$

-   La part d'information portée par le plan $k_1, k_2$ est donnée par
    $$\frac{\lambda_{k_1} +\lambda_{k_2}}{\sum_{k=1}^p\lambda_k}$$

## Attention aux unités - Illustration sur les données de manchots

```{r PCA_manchots_2, eval = TRUE, echo = TRUE}
#| echo: true
#| classes: custom3565
#| output-location: column
#| results: hold
#| message: false

library(FactoMineR)

penguins.PCA <- PCA(X = penguins,
                    scale.unit = FALSE, ## should we scale the data ?
                    ncp = 4, ## number of components : as many as initial variables
                    graph = FALSE, ## no graph right now
                    quali.sup = c(1, 2, 7,8)) ## to indicate the column number of qualitative variable
penguins.PCA$eig

penguins.PCA$svd$V ## what is named P in the lecture
```

La dispersion du nuage projeté sur le premier axe représente $6.45 10^5$
en terme d'inertie soit $99.98%$ de l'inertie totale.

. . .

[Trop beau pour être vrai ?]{.rouge}

Réfléchissons aux données, ce qui est crucial dans toutes analyses de
données.

```{r PCA_manchots_desc, eval = TRUE, echo = TRUE}
#| echo: true
#| classes: custom3565
#| output-location: column
#| results: hold
#| message: false


penguins %>%
  summarise(across(where(is.numeric), list(mean = mean, sd= sd)))  |>
  pivot_longer(cols = everything(),  names_to = c("Var", "Resume"),
      names_pattern = "(.*)_(mean|sd)") |>
  pivot_wider(names_from = Resume,values_from = value)
```

. . .

Les variables ont des ordres de grandeur variable.

La contribution à la distance entre individu de la variable `Mass`est
plus importante que celle de la variable `bill_length_mm`.

[Est-ce souhaitable ?]{.rouge}

## Attention aux unités - Illustration sur les données de manchots normalisées

```{r PCA_manchots_3, eval = TRUE, echo = TRUE}
#| echo: true
#| classes: custom3565
#| output-location: column
#| results: hold
#| message: false

penguins.PCA <- PCA(X = penguins,
                    scale.unit = TRUE, ## should we scale the data,
                    ncp = 4, ## number of components : as many as initial variables
                    graph = FALSE, ## no graph right now
                    quali.sup = c(1, 2, 7,8)) ## to indicate the column number of qualitative variable
penguins.PCA$eig

penguins.PCA$svd$V ## what is named P in the lecture
```

On a centré réduit les variables pour les rendre comparables et leur
donner la même importance.

Dans ce cas l'inertie totale est le nombre de variables

Lorsque l'on veut s'abstraire des unités, on centre te on réduit les
données pour réaliser une [ACP normée]{.rouge}.

## Choix de la distance

L'ACP normée (ACP sur les données centrées et réduites) peut-être
envisagée comme une ACP avec une autre distance que la distance
euclidienne.

Une autre manière de présenter la même approche est de dire qu'on change
la métrique utilisée

::: {.callout-note icon="false" appearance="minimal"}
### Definition (métrique)

Une métrique $M$ permet de définir une distance entre les observations
(ainsi, M est une matrice de ${\mathbb{R}}^p\times{\mathbb{R}}^p$
symétrique, définie et positive).
:::

### Calcul de distance

Soient $\boldsymbol{x}_1 \in {\mathbb{R}}^p$ et
$\boldsymbol{x}_é\in{\mathbb{R}}^p$, la distance entre
$\boldsymbol{x}_1$ et $\boldsymbol{x}_2$pour la métrique $M$ est
$$d_M(\boldsymbol{x}_1, \boldsymbol{x}_2)^2= \lVert\boldsymbol{x}_1 - \boldsymbol{x}_2\rVert_M^2 = (\boldsymbol{x}_1 - \boldsymbol{x}_2)^\top M (\boldsymbol{x}_1 - \boldsymbol{x}_2).$$

::: panel-tabset
### Exemple $I_p$

Si $M= I_p$,

$$d_M(\boldsymbol{x}_1, \boldsymbol{x}_2)^2= \lVert\boldsymbol{x}_1 - \boldsymbol{x}_2\rVert_M^2 = (\boldsymbol{x}_1 - \boldsymbol{x}_2)^\top I_p (\boldsymbol{x}_1 - \boldsymbol{x}_2) =\sum_{k=1}^p (x_{1k}-x_{2k})^2.$$

### Distance normalisant les variables

Si $M$ est diagonale est $M_{ii}= 1/ s_i^2$ où $s_i^2$ est la variance
de la variable $i$,

$$d_M(\boldsymbol{x}_1, \boldsymbol{x}_2)^2=  (\boldsymbol{x}_1 - \boldsymbol{x}_2)^\top M (\boldsymbol{x}_1 - \boldsymbol{x}_2) =\sum_{k=1}^p \frac{(x_{1k}-x_{2k})^2}{s_k^2}=\sum_{k=1}^p \left ( \frac{x_{1k}}{s_k}-\frac{x_{2k}}{s_k} \right)^2.$$

Dans une ACP normée on s'intéresse donc aux valeurs propres de la
matrice $M^{1/2} X^\top  W X M^{1/2},$ il s'agit de la matrice de corrélation.

On note $V$ la matrice de covariance et on va donc travailler sur $M^{1/2} V M^{1/2}$
:::


## Identifier les directions principales pour une métrique quelconque $M$

### Maximiser $I_{u_1^\perp}$
sous la contrainte $\lVert\boldsymbol{u_1}\rVert$ \begin{align}

 I_{u_1^\perp} & = \frac{1}{n}\lVert X  \boldsymbol{u_1}\rVert^2_M \\
        & = \frac{1}{n} (X  \boldsymbol{u_1})^\top M (X  \boldsymbol{u_1})\\
        & =   \boldsymbol{u_1}^\top M^{1/2} \left (\frac{1}{n} X^\top X\right)  M^{1/2} \boldsymbol{u_1}
\end{align}

D'apères ce qui précède [$\boldsymbol{u_1}$ est le premier vecteur propre]{.rouge} de
$M^{1/2} V M^{1/2}.$

[Le changement de métrique a un impact sur la matrice à diagonaliser mais pas sur la logique de construction.]{.rouge}



# Etude du nuage des individus

## Représentation des individus - Composantes principales

::: {.callout-note icon="false" appearance="minimal"}
### Definition (Composantes principales)

Les coordonnées des $n$ observations sur les $p$ axes factoriels sont
groupées dans la matrice $C$: $c_{ik}$ est la coordonnée de
l'observation $i$ sur l'axe $\Delta_{\boldsymbol{u_k}}$. La $k$-ème
composante principale est la colonne $k$ de la matrice $C$, notée $C^k$.
$$
C = [\class{orange}{C^1} | \ldots | \class{orange}{C^p}] \text{ avec } \class{orange}{C^k}=\begin{pmatrix}
c_{1k}\\
\vdots\\
c_{nk}
\end{pmatrix}.
$$
:::

::: panel-tabset
### Vectoriellement

On a les expressions suivantes $$
C^k=Xu_k \mbox{ et } C=XP,
$$ où $P$ désigne la matrice de changement base, i.e la matrice
contenant les vecteurs propres.

### Visuellement

![](change_baseXtX_acp.png){width="50%"}

### Remarques

-   Les $C^k$ sont des combinaisons linéaires des variables de départ.
-   Elles sont centrées, de variance
    $\lambda_k=\sum_{i=1}^n w_i c_{ik}^2$ et non-corrélées 2 à 2.
-   Les $C^k$ sont vecteurs propres (non unitaires) de $\frac{1}{n}X W X^\top$ de valeur propre
    $\lambda_k$.
:::

## Représentation des manchots

```{r CompPrinc_manchots, eval = TRUE, echo = TRUE}
#| echo: true
#| classes: custom3565
#| output-location: column
#| results: hold
#| message: false


X <- penguins |>
  mutate(year = as.factor(year))|>  ## year willnot be considered as numerical variable
  select(where(is.numeric)) |>      ## select all numeric columns
  mutate_all(list(scale2)) |>          ## and scale them
  as.matrix()

n <- nrow(X) # number of individuals
d <- ncol(X)

penguins_svd <- svd( 1/n * t(X)%*% X ) # to get P and D from t(X)WX

## eigenvalues
penguins_eigenvalue <- penguins_svd$d

P <- penguins_svd$v
cp <- X %*% P
head(cp[, 1:2])
cat("\n")
head(penguins.PCA$ind$coord[,1:2])

#plot(penguins.PCA, choix = "ind", axes = c(1,2),invisible=c('ind.sup'),label =c('ind') )
library(factoextra)
fviz_pca_ind(penguins.PCA, label="none", addEllipses=FALSE)
```

## Qualité de la représentation d'un individu

-   Même avec ces précautions, on déforme le nuage de points
-   Certains sont plus déformés que d'autre, il faut les identifier

Un individu est d'autant moins déformé qu'il est proche du plan sur
lequel on le projette, ce qui est donné par l'angle entre le vecteur
initial et le vteur projeté

::::: columns
::: {.column width="55%"}
```{r angle_ind}
#| echo: false
#| message: false


individus <- data.frame(
  x = c(1, -2),   # Coordonnées des points individuels
  y = c(0.9, -0.5)
)

u = c(1,2)
norme_u = sum(u^2)

ind_proj <- (as.matrix(individus)%*% as.matrix(u, ncol= 1)/(norme_u)) %*% matrix(u, ncol = 2, nrow=1)
ind_proj <- as.data.frame(ind_proj) %>%
  rename(xend= V1, yend=V2)
individus_dessin <- cbind(individus, ind_proj) %>% as.data.frame()

# Schéma avec ggplot
ggplot() +
  geom_segment(aes(x = -1, y = -2, xend = 1, yend = 2), arrow = arrow(length = unit(0.3, "cm")), linewidth = 0.5) +  # Axe vertical
  geom_point(data = individus, aes(x = x, y = y), size = 3, col= "#398d55") +
  geom_segment(data = individus_dessin, aes(x = x, y = y, xend = xend, yend = yend), linetype = "dashed", alpha = 0.2) +
  geom_segment(data = individus_dessin, aes(x = x, y = y, xend = 0, yend = 0), linetype = "dotted") +

  geom_text(aes(x = 0.3, y = 2), label = as.character(expression(R^p)),col="#F7A913", size = 5) +
  geom_text(aes(x = -1.2, y = -2), label = as.character(expression(Delta)), vjust = -0.8, size = 5) +
  geom_text(data= individus_dessin, aes(x = xend, y =yend),  label = paste0("p[", 1:2, "]^Delta"), parse = TRUE, vjust = -0.8, size = 5) +
  geom_text(data= individus_dessin, aes(x = x, y =y),  label = paste0("x[", 1:2, "]"), parse = TRUE, vjust = -0.8, size = 5, col = "#398d55") +
  geom_text(data= individus_dessin, x=c(0.5, -0.4), y =c(0.3, -0.5),  label = paste0("theta[", 1:2, "]"), parse = TRUE, vjust = -0.8, size = 5, col = "#398d55") +
  geom_text(aes(x = 0.15, y = -0.1), label = "G", vjust = -0.8, size = 5) +
  geom_point(aes(x = 0, y = 0),  size = 2) +
  geom_point(data = individus_dessin, aes(x = xend, y = yend),  size = 2) +
  theme_minimal()  +
  coord_fixed() +  # Conserver un rapport d'aspect fixe pour les axes
  theme (axis.title=element_blank(),
         axis.text=element_blank(),
         axis.ticks=element_blank(),
         panel.grid = element_blank())

```
:::

::: {.column width="40%"}
\begin{align}
<\class{alea}{x_i}, \boldsymbol{u_1}> &= \lVert\class{alea}{x_i}\rVert \lVert\boldsymbol{u_1}\rVert cos(\theta_i) \\
& = \class{alea}{x_i}^\top \boldsymbol{u_1}, \quad \left( \text{ou } \class{alea}{x_i}^\top M \boldsymbol{u_1}  \text{avec une autre métrique} \right) \\
\end{align}

$$\cos^2(\theta_i) = \frac{(\class{alea}{x_i}^\top \boldsymbol{u_1})^2}{\lVert\class{alea}{x_i}\rVert^2}$$
:::
:::::

[Interprétation]{.rouge}: plus $\cos^2(\theta_i)$ est proche de $1$,
mieux l'individu est représenté.

## Contribution d'un individu à la définition de l'axe

-   Les individus sont plus ou moins important dans la définition des
    axes

:::::: panel-tabset
### Intuitivement

::::: columns
::: {.column width="55%"}
```{r contrib_ind}
#| echo: false
#| message: false

individus <- data.frame(
  x = c(1,   -1,    0.2,   0.3, -0.2, 1.5),   # Coordonnées des points individuels
  y = c(0.9, -2.4, -0.1, 0.3, -0.4, 2.8)
) |>
  mutate(x = x- mean(x), y = y -mean(y))


u = svd(as.matrix(individus))$v[,1]
norme_u = sum(u^2)

# Schéma avec ggplot
ggplot() +
  #geom_segment(aes(x = -2*u[1], y = -2*u[2], xend = 2*u[1], yend = 2*u[2]), linewidth = 0.5) +  # Axe vertical
  geom_point(data = individus, aes(x = x, y = y), size = 3, col= "#398d55") +
  # geom_text(aes(x = -1.2, y = -2), label = as.character(expression(Delta)), vjust = -0.8, size = 5) +
  geom_text(data= individus, aes(x = x, y =y),  label = paste0("x[", 1:6, "]"), parse = TRUE, vjust = -0.8, size = 5, col = "#398d55") +
  geom_text(aes(x = 0.15, y = -0.1), label = "G", vjust = -0.8, size = 5) +
  geom_point(aes(x = 0, y = 0),  size = 2) +
  theme_minimal()  +
  coord_fixed() +  # Conserver un rapport d'aspect fixe pour les axes
  theme (axis.title=element_blank(),
         axis.text=element_blank(),
         axis.ticks=element_blank(),
         panel.grid = element_blank())

```
:::

::: {.column width="40%"}
[Quels sont les points qui vont le plus contribuer à la détermination
des axes ?]{.rouge}
:::
:::::

### Formellement

La [contribution relative de l’observation]{.rouge} $\class{alea}{x_i}$
à la construction de l’axe principal $k$ est la part d’inertie de l’axe
$k$ expliquée par $\class{alea}{x_i}$ donnée par

$$Ctr(i, k) = \frac{1}{n} \frac{c_{ik}^2}{\lambda_k}.$$ En effet
$\lambda_k = \frac{1}{n} \sum_{i=1}^n c_{ik}^2$ (ie. la variance du
nuage projeté).

Remarque : si les individus ont des poids $w_i$ alors
$$Ctr(i, k) =  \frac{w_i c_{ik}^2}{\lambda_k}.$$
::::::

# Etude des variables

## Qualité de la représentation d'une variable sur un plan factoriel

:::::: panel-tabset
### Intuitivement

```{r angle_var}
#| echo: false
#| message: false
a <- 0.5
b <- 0.2
lambda1 <- 1.6
lambda2 <- 0.4

variables <- data.frame(
  xend = c(a/sqrt(a^2+b^2),  1, 0.5/sqrt(4.25), -sqrt(2)/2),   # Coordonnées des points individuels
  yend = c(b/sqrt(a^2+b^2), 0, 2/sqrt(4.25), -sqrt(2)/2),
  x0 = c(0,0, 0, 0),
  y0= c(0,0, 0, 0)
)

pos <- data.frame(x=0.5, y = 0.06)

# Schéma avec ggplot
ggplot() +
  #geom_segment(aes(x = -2*u[1], y = -2*u[2], xend = 2*u[1], yend = 2*u[2]), linewidth = 0.5) +  # Axe vertical
  geom_segment(data = variables, aes(xend = xend, yend = yend, x= x0, y = y0), arrow = arrow(length = unit(0.3, "cm")),   col= c("#F7A913", "#d61d09","#F7A913","#d61d09")) +
  geom_segment(aes(xend = lambda1 * variables$xend[2], yend = lambda1 * variables$yend[2], x= 0, y = 0), arrow = arrow(length = unit(0.3, "cm")),   col= "#d61d09", alpha = 0.5) +
  geom_text(aes(x = lambda1 * variables$xend[2], y = lambda1 * variables$yend[2]), label = as.character(expression(C^k == lambda[k]~u^k)),   col= "#d61d09", parse = TRUE, alpha = 0.5) +
  # geom_text(aes(x = -1.2, y = -2), label = as.character(expression(Delta)), vjust = -0.8, size = 5) +
  geom_text(data= variables, aes(x = xend, y =yend),  label = c("X^j", "u^k", "X^i", "u^l"), parse = TRUE, vjust = -0.1, hjust =  -0.5,  size = 5,   col= c("#F7A913", "#d61d09", "#F7A913", "#d61d09"))  +
  geom_text(data = pos, aes(x = x, y = y), label = as.character(expression(theta["j,k"])), parse = TRUE,  size = 5) +
  geom_text(data = pos, aes(x = x/3, y = 3*y), label = as.character(expression(theta["i,k"])), parse = TRUE, size = 5) +
  theme_minimal()  +
  coord_fixed() +  # Conserver un rapport d'aspect fixe pour les axes
  theme (axis.title=element_blank(),
         axis.text=element_blank(),
         axis.ticks=element_blank(),
         panel.grid = element_blank())

```

### Formellement

::::: columns
::: {.column width="45%"}
![](change_baseXtX_acp.png)
:::

::: {.column width="45%"}
-   Une variable est bien représentée sur l'axe $k$ si l'angle entre
    cette variable et la composante $C^k$ est petit, son cosinus est
    proche de 1 ou -1.

Or $<X^j, C^k> = \lVert X^j\rVert\lVert C^k\rVert cos(\theta_{jk})$

\begin{align}
<X^j, C^k> & = <(CP^\top)^j, C^k> \\
& = \underset{= (P C^\top)_j}{\underbrace{\left((CP^\top)^j\right)^\top}}   C^k \\
\end{align}

Ainsi $<X^j, C^k>$ est le $j^\text{ème}$ terme du vecteur $PC^\top C^k$,
or $C^\top C$ est diagonale avec $ \lVert C_k \rVert  = n \lambda_k$, donc
$(C^\top C)^k=(0, \dots, n\lambda_k, 0, \ldots)^\top$ et

$$\cos(\theta_{jk}) = \frac{n \lambda_k \boldsymbol{u_k^j}}{\sqrt{\lVert X^j\rVert^2 \, \lVert C_k\rVert^2}}=  \sqrt{\lambda_k} \boldsymbol{u_k^j},  \quad \cos^2(\theta_{jk}) =  \lambda_k{(\boldsymbol{u_k^j})^2}$$
:::
:::::
::::::

## Ajouter des informations supplémentaires

Il est possible d'ajouter des informations supplémentaires

### Des individus supplémentaires qui ne seront pas utilisés pour construire les axes

Si on veut ajouter un nouvel individu $x_{new}$

-   on calcule $\boldsymbol{x}_{new} - \boldsymbol{g}$, le point centré
    (et potentiellement on divise par l'écart type des variables
    calculés sur le jeu de données initial)

-   On projette sur les axes principaux :
    $(\boldsymbol{x}_{new} - \boldsymbol{g} )^\top \boldsymbol{u_k}$

### Une variable supplémentaire

Si on souhaite ajouter une nouvelle variable
$\boldsymbol{x}^{new}= (x^{new}_1, \ldots, x^{new}_n)^\top \in {\mathbb{R}}^n$

-   On soustrait sa moyenne
    $\tilde{\boldsymbol{x}}^{new} = \boldsymbol{x}^{new} - m^{new}$,
    $m^{new}=\frac{1}{n} \sum_{i=1}^n x^{new}_i$ (ou
    $m^{new}= \sum_{i=1}^n w_i x^{new}_i$ si toutes les observations
    n'ont pas le même poids)

-   On projette $\tilde{\boldsymbol{x}}^{new}$ sur $\boldsymbol{u_k},$

$$Cor(\tilde{\boldsymbol{x}}^{new}, u_k) = \frac{(\tilde{\boldsymbol{x}}^{new})^\top W \boldsymbol{u_k} }{\lVert\tilde{\boldsymbol{x}}^{new}\rVert}$$

## Retournons à nos manchots

```{r PCA_manchots_final, eval = TRUE, echo = TRUE}
#| echo: true
#| classes: custom3565
#| output-location: column
#| results: hold
#| message: false

penguins.PCA <- PCA(X = penguins,
                    scale.unit = TRUE, ## should we scale the data,
                    ncp = 4, ## number of components : as many as initial variables
                    quali.sup = c(1, 2, 7,8)) ## to indicate the column number of qualitative variable

 # library(Factoshiny)
 # PCAshiny(penguins)
```

# Comprendre les données du Doubs

## Présentation des données

## L'exemple des caractéristiques du Doubs

On a mesuré les caractéristiques physico chimiques sur 30 sites
différents le long de la rivière Doubs.

::::: columns
::: {.column width="48%"}
Les 6 premières lignes (parmi 30) du jeu de données doubs.env

```{r extrait_doubs, echo = FALSE, eval = TRUE}
doubs.env %>% print(n=6)
```
:::

::: {.column width="48%"}
-   das : distance à la source ( $km$ ),
-   alt : altitude ( $m$ )
-   pen : la pente (dénivelé pour 1000m)
-   deb : le débit () $m^3.s^{-1}$ )
-   pH : le pH de l'eau,
-   dur : la concentration en calcium ( $mg.L^{-1}$ ),
-   pho : concentration en phosphate ( $mg.L^{-1}$ ),
-   nit : concentration en nitrate ( $mg.L^{-1}$ ),
-   amn : concentration en ammonium ( $mg.L^{-1}$ ),
-   oxy : concentration d'oxygène dissous ( $mg.L^{-1}$ ),
-   dbo : Demande biologique en oxygène ( $mg.L^{-1}$ ).
:::
:::::

[Comment visualiser au mieux ces données pour faire apparaître les liens
entre variables et identifier des ressemblances entre individus
?]{.rouge}

## Analyse par ACP

-   Poids des individus ?
-   Quelle distance entre les variables ?
-   Lancer l'analyse
    -   quelles sont les composantes intéressantes (règle empririque du
        choix du nombre d'axes) ?
    -   quelles informations sont visibles sur le premier plan factoriel
        ?
    -   Que peut on dire des liens entre les variables ?
    -   A votre avis quels sites contribuent le plus à la formation de
        l'axe 1 ?
    -   Que peut on dire du pH ?

# L'ACP comme une méthode de compression

## Ne garder que l'information pertinente

On a utilisé l'ACP comme une technique de visualisation, mais on peut
sen servir comme une technique de compression (avec perte). Il suffit de
ne garder que les première composantes principales.

[L'exemple sur une image](ACP_images.html)

# Point Technique

## Matrice d'inertie

::::: columns
::: {.column width="49%"}
### Revisiter l'inertie

L'inertie du nuage de points est définie comme
$$I = \sum_{i=1}^n w_i \lVert\class{alea}{x_i} -\boldsymbol{g}\rVert^2_M$$
On a considéré des variables centrées donc
$\boldsymbol{g} = \boldsymbol{0}$.

-   Or

\begin{align}
 I = & \sum_{i=1}^n w_i x_i^\top M x_i = tr(\sum_{i=1}^n w_i x_i^\top M x_i ) \\
   = & tr(\sum_{i=1}^n M x_i w_i x_i^\top ) \quad \text{(commutatitvité de la trace)} \\
   = & tr(M \sum_{i=1}^n  x_i w_i x_i^\top )\\
   = & tr(M X^\top W X)\\
   = & tr( X^\top W X M)\\
   = & tr( V M)\\

\end{align}
:::

::: {.column width="49%"}
### Exemple sur les cas présentés

-   Dans le cas de l'ACP, la matrice $V$ est la matrice de covariance et
    $M = Id$,

-   Dans le cas de l'ACP normée , la matrice $V$ est la matrice de
    covariance et $M = Diag(s_k^{-2})$,
:::
:::::

# Bilan

## Ce que vous pensez devoir retenir

-   l'ACP permet de mettre en valeur l'information principale contenu
    dans un jeu de données

    -   les relations entre variables
    -   les proximités entre individus

-   Cette méthode permet d'obtenir une projection sur queluqes axes
    indépendants et bien choisis, au sens ou ils résument le mieux
    l'information

-   Techniquement l'ACP consiste à diagonaliser la matrice de covariance

-   Les axes sont définis par les vecteurs propres de la matrice\
    $$ X^\top W X$$

-   Ils sont rangés par ordre d'importance décroissante, au sens où le
    premier est celui qui préserve le plus l'inertie du nuage projeté
    (i.e a la plus grande valeur propre)

-   Il faut adapter la métrique au probleme

-   l'information portée par un axe est mesurée par la valeur propre
    associé à l'axe

-   il faut savoir interpréter le nuage des individus, éclairés par les
    liens entre axes principaux et anciennes variables
