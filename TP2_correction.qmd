---
title: "Labs2 - Analyse en composante Principale et au-delà — Corrigé"
author:
  - name: Marie-Pierre Etienne
    affiliation:
      - ENSAI - CREST
    email: marie-pierre.etienne@ensai.fr
date: "Last updated on `r format(Sys.time(), '%d %B, %Y')`"
institute: https://marieetienne.github.io/MAF/
bibliography: TPs.bib
editor:
  markdown:
    wrap: 72
css: mpe_pres_revealjs.css
format:
  html: default
  pdf: default
---

```{r}
#| label: setup_hid
#| include: false
#| eval: true

library(tidyverse)
library(wesanderson)
knitr::opts_chunk$set(echo = TRUE, comment = NA, cache = TRUE, message = FALSE,
                      warning = FALSE, eval = TRUE,
                      fig.align = "center")
theme_set(theme_minimal())
options(ggplot2.discrete.colour=   scale_color_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) )
couleur <-  wesanderson::wes_palette(name = "Darjeeling1")
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / ( sqrt((length(x)-1) / length(x)) *sd(x, na.rm) )
```

::: hidden
\$\$

\newcommand\R{{\mathbb{R}}}
\newcommand\Xbf{{\boldsymbol{X}}}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\xcol[1]{\boldsymbol{x}^{#1}}
\newcommand\xrow[1]{\boldsymbol{x}_{#1}}
\newcommand\xbf{\boldsymbol{x}}
\newcommand\ybf{\boldsymbol{y}}

\$\$
:::



# Statistiques avec R à l'Ensai

Le Logiciel R est libre et disponible sur divers plateformes [sur cette page](https://www.r-project.org/) et le logiciel [Rstudio](https://posit.co/download/rstudio-desktop/) est une interface agréable pour l'utilisation de R  Vous pouvez installer ces logiciels sur votre ordinateur, toutefois il faut être prêt à gérer soi-même les problèmes de version, d'installation etc.

Pour plus de sérénité il est préférable d'utiliser les serveurs auqxuels vous pouvez vous connecter par l'url suivante (depuis n'importe où)

[http://clust-n1.ensai.fr \; ou \; http://clust-n2.ensai.fr]{.center}

Pour accéder en plus à une interface graphique, par exemple pour utiliser `shiny` et notamment `Factoshiny`, il faut être à l'ENSAI et utiliser l'adresse


[http://clust-n1.domensai.ecole \; ou \; http://clust-n2.domensai.ecole]{.center}

# De bonnes pratiques

## Préparation de l'environnement

* Ouvrir un navigateur web et se connecter au serveur de calcul.
* Créer un Projet  (File --> New Project) ou bien en cliquant en haut à droite.  Le travail par projet facilite l'accès aux données (notamment lorsque l'on passe d'un ordinateur à l'autre) et permet surtout de "zapper" d'un projet à un autre.
* Choisissez Créer un projet depuis un Nouveau Repertoire, nommez le `MAF_TD2` (comme toujours en programmation on évite les caractères spéciaux, c'est-à-dire, les accents, les espaces entre autres)
* Pour pouvoir garder le code et l'interprétation des résultats au même endroit nous allons utiliser un type de document [Quarto](https://quarto.org/) qui est adapté pour mélanger code et texte. Quarto accepte du code R mais aussi Python et Julia. Créer un fichier `TP2.qmd`. Vous pouvez utiliser [le fichier d'exemple](TP_MAF_exemple.qmd).
* Passez en mode `source`(en haut à gauche) au lieu de `Visual` ceci permet de mieux comprendre la structure d'un document quarto.

## Gestion de packages

Nous utilisons la suite de packages `tidyverse` pour la manipulation de données, `FactomineR` pour la mise en oeuvre des analyses factorielles et éventiuellement `factoextra` pour des sorties plus jolies. (pour une exploration plus interactive on pourra aussi utiliser `Factoshiny` ou `explor`).


::: callout-tip
 Une bonne pratique consiste à ne charger que le nombre minimal de packages nécessaires. Evitez de commencer tous les codes avec la même liste de package, pesez l'intérêt de chacun d'eux.
:::


## Structure Document Quarto

Un document Quarto se divise en plusieurs sections clés qui facilitent l'organisation de l'analyse, l'intégration de code, et la présentation des résultats. Voici une présentation des principales composantes :

### En-tête YAML

C'est la section de métadonnées du document. Elle est située en haut du fichier et est encadrée par des lignes ---. On y définit des informations essentielles telles que :
        title : le titre du document ;
        author : l’auteur ou les auteurs du document ;
        date : la date de création, qui peut être dynamique avec R, par exemple en utilisant date: "r Sys.Date()" ;
        format : le format de sortie (HTML, PDF, Word, revealjs pour les présentations, etc.).

    Exemple d'en-tête YAML :

```{yaml}
#| echo: true

---
title: "Analyse de Données avec R"
author: "Votre Nom"
date: "`r Sys.Date()`"
format: html
---

```

### Texte en Markdown

La rédaction du texte dans Quarto utilise la syntaxe Markdown pour structurer le contenu de manière simple et lisible. Markdown permet de formater le texte (titres, listes, liens, etc.) sans avoir à écrire du code complexe. Par exemple :

```{markdown}
    # pour les titres (niveau 1, 2, etc. avec ##, ###...),
    - ou * pour les listes,
    [lien](url) pour les liens.

```

Exemple :

```{markdown}
#| echo: true

## Introduction
Ce document présente une analyse de données en utilisant **R**.
- Point 1
- Point 2
```

Le markup langage markdown permet aussi d'insérer des formules mathématiques, comme du latex.

```{markdown}
  $$x_{+k}= \sum_{i=1^n} x_{ik}$$

```

### Blocs de Code
Quarto permet d'intégrer du code R (mais aussi  Python, et Julia) grâce à des blocs de code, `chunks`. Les blocs de code sont délimités par trois accents graves (triple backtick), avec le langage spécifié entre accolades ({}). Les blocs peuvent exécuter du code directement dans le document et afficher les résultats en ligne.

Exemple de bloc de code en R :

```{markdown}
 ```{r chunk_example}
 #| echo: true

 # Calcul simple
  2+2
  ```
```

Les options de rendu du bloc de code peuvent être précisées. Par exemple ici
* #| echo: true permet de montrer le code
* #| eval: false permet de ne pas executer le code

Une présentation plus complète des options, pour un rendu plus fin est disponible sur [le site de Quarto](https://quarto.org/docs/computations/r.html#chunk-options).

Le bloc peut être défini en tapant
  * manuellement les triples backticks,
  * ou en utilisant le menu Code --> Insert Chunk,
  * ou avec le raccourci CTRL + ALT + i


### Rendu et exportation du Document

Une fois le document terminé, on peut le rendre dans le format souhaité en utilisant la commande suivante dans le terminal `quarto render mon_document.qmd`ou en cliquant sur Render dans RStudio.

Cette commande crée une version finalisée du document dans le format spécifié (HTML, PDF, etc.).


Avec Quarto on peut tout autant faire des slides, des rapports en pdf, en word etc. Ceci permet de s'assurer que les résulats présentés sont ceux de l'analyse et c'est un *premier pas important dans le sens d'assurer plus de reproductibilité en science*. (d'autres outils sont utiles voir [ici](https://marieetienne.github.io/reproductibilite/_presentation/#1) )

C'est très utile en TP puisque vous pouvez commenter les analyses proposées, les résultats obtenus directement dans le fichier qui contient les codes permettant d'obetenir ces résultats.


Plus largement, en suivant cette structure, vous pouvez organiser vos analyses de manière professionnelle et claire.

### Création du document

Lors de la mise en place de l'analyse, on peut éxécuter chaque chunk avec le raccourci CTL + Enter, ou en cliquant sur la petite flèxche verte en haut à droite du chunk.




# Le jour du dépassement


## Les packages que nous allons utiliser pour l'ACP

```{r}
#| label: load_package
library(FactoMineR)
library(factoextra)
library(explor)
library(tidyverse)
library(ggpubr)
```

Les méthodes d'analyse factorielles sont utiles pour l'exploration de
données, soit en tant que telle soit dans une phase préparatoire à un
travail de modélisation. Il est utile de disposer à la fois d'outils
interractifs pour l'exploration, mais également de garder une trace de
l'exploration avec un document bilan.

Nous allons dans ce TP utiliser un fichier `TP2.qmd` pour garder une
trace des analyses faites mais nous pourrons en parallèle utiliser le
potentiel d'une exploration plus interactive soit avec `Factoshiny` soit
avec `explor` selon ce que vous préférez.

Dans ce TP nous allons explorer un premier exemple en détail avec une
démarche bien guidée tandis que le second exemple a pour objectif de
vous laisser face aux données en autonomie, pour vérifier que vous voyez
comment aborder un jeu de données avec les méthodes d'analyse
factorielle.

## Description des données

Le jeu de données utilisé dans ce premier cas pratique provient du [site
web de Global Footprint
Network](https://www.footprintnetwork.org/licenses/public-data-package-free/).
Il contient les résultats d'empreinte écologique et de biocapacité pour
184 pays.

Les données sont [disponibles sur ce
lien](https://marieetienne.github.io/datasets/overshootday_overview.csv).

### Quelques définitions

Le calcul de l'empreinte écologique et de la biocapacité nous aide à
répondre à la question de recherche fondamentale : Quelle est la demande
des êtres humains envers les surfaces biologiquement productives
(empreinte écologique) par rapport à la quantité que la planète (ou la
surface productive d'une région) peut régénérer sur ces surfaces
(biocapacité) ?

-   Hectare global (gha) : C'est l'unité choisie pour exprimer toutes
    les quantités d'intérêt concernant la consommation/émission de
    carbone. Une unité de surface correspondant à la productivité
    moyenne d'un hectare de terres mondiales. Un hectare de terres
    agricoles vaudra plus d'hectares globaux qu'un hectare de désert.
-   Empreinte écologique (en gha par personne) : Le nombre de gha requis
    pour produire les besoins et absorber les déchets d'un pays.
-   Biocapacité (en gha) : La capacité d'un pays à produire ce dont il a
    besoin et à absorber ses déchets (réserve écologique).
-   Jour de dépassement : Jour de l'année où la demande d'un pays
    dépasse sa biocapacité annuelle.

Des détails supplémentaires sont disponibles
[ici](https://data.footprintnetwork.org/?_ga=2.237587203.280109455.1689844989-712229654.1682588383#/abouttheData)

## Objectifs

Les relations entre les différentes mesures de l'empreinte écologique et
les caractéristiques des pays.

### Chargement des Données

```{r}
#| label: load_data
#| echo: false
#| eval: true
#| output: false
# Chargement des données
overshoot_dta <- read.table("https://marieetienne.github.io/datasets/overshootday_overview.csv",
                            sep = ",",
                            header = TRUE,
                            row.names = 1)


```
### Analyse Exploratoire des Données

-   Pouvez vous indiquer le nombre de lignes et de colonnes

::: correction
Le jeu de données contient **184 lignes et 12 colonnes** avant suppression des valeurs manquantes.\
Il  y a  **162 pays complets**.\
:::

### Statistiques descriptives :

Proposez des résumés numériques pour chaque variable, ainsi qu'une étude
des corrélations.

```{r}
#| label: desc_data
#| echo: true

overshoot_dta |>
  mutate(across(c(region, income_group), ~as.factor(.x))) |>
  summary()
```

Pour le moment, on va supprimer les données manquantes (on verra par la
suite comment gérer ce problème de manière plus satisfaisante). Pour
ceci on peut utiliser `drop_na` du package tidyverse ou `na.omit` de R
Base.

```{r}
#| label: drop_na
#| echo: true
#| include: false

overshoot_dta <- overshoot_dta |> drop_na()
summary(overshoot_dta)
```

## Construire une ACP



-   Quel poids choisir pour les pays ?

::: correction
Si on veut faire un bilan à l'échelle de la planète il est pertinent de prendre les poids des pays proportionnels à leur population. Ainsi un pays avec une grosse population aura plus de poids qu'un petit pays.  Si on souhaite plutôt comparer les pays, alors on va pas mettre la population comme poids, pour s'assurer de représenter équitablement tous les pays. Il est utile de faire l'exercice avec et sans poids pour constater les différences.
:::



-   Quelle métrique choisir ?

::: correction
On va normaliser puisque espérance de vie n'a pas de lien avec l'empreinte écologique
:::

-   Quelles variables met-on en variables supplémentaires ?

::: correction
Les variables qualitatives ici region et income_group sont forcément supplémentaires.
Par ailleurs, le nombre de terre est parfaitement corrélée à la production par construction (nombre de terres = production / prodction_terre), donc cette dimension est inutile. On devrait trouver que la 10eme valeur propre vaut 0  si on met cette variable.
:::

```{r}
#| label: pca
#| echo: true
#| include: false
overshoot_pca <- PCA(overshoot_dta,
                     scale.unit = TRUE,
                     quali.sup = c(4,5), quanti.sup = c(11))
overshoot_pca$eig
fviz_eig(overshoot_pca)
```

Si on veut tester avec des poids


```{r}
#avec la population comme poids
overshoot_pca_w <- PCA(overshoot_dta,
                     scale.unit = TRUE,
                     quali.sup = c(4,5),
                     row.w = overshoot_dta[,6],
                     quanti.sup = c(6, 11))

```

-   Quelle est l'inertie portée par l'axe 1 , l'axe 2, l'axe 3 ?

```{r}
#| label: inertia
overshoot_pca$eig[1:3,1]
```

::: correction
L'inertie vaut 4.88 sur le premier axe, 1.26 sur le second et 0.99 sur le 3eme.
Le premier axe explique donc \~54%, le second \~14%, le troisième \~11%.   \~80% cumulés.
:::



-   Commentez la qualité de la représentation sur le plan (1-2), puis
    (1-3).

```{r}
plot(overshoot_pca, choix = "var", axes = c(1,2))
plot(overshoot_pca, choix = "var", axes = c(1,3))
plot(overshoot_pca, choix = "var", axes = c(2,4))
```

::: correction
Le plan 1-2 représente  `r overshoot_pca$eig[2,3]` d'inertie, tandis que le plan 1-3 en représente `r sum(overshoot_pca$eig[c(1,3),2])`. Le premier axe porte à lui seul `r overshoot_pca$eig[1,3]` d'inertie.

La plupart des variables sont assez bien représentées sur ce plan à l'exception de la population, dans une moindre mesure le nombre de pays nécessaires. Il faut regarder sur le plan 2,4 pour bien voir biocapacité et nombre de pays requis.


Lorsque l'on fait une ACP en prenant la variable population pop en compte et sans mettre de poids, on voit que l'axe 3 est la population, cela signifie que la population d'un pays est indépendante des 2 premiers axes dans la suite on pourrait déduire qu'il y a de gros pays avec une grosse empreinte carbone mais aussi des gros pays avec une faible empreinte carbone.
:::

-   De quelles variables nous parle l'axe 1 ? l'axe 2 ? Pour répondre à
    cette question on regardera le cercle des corrélations mais aussi
    les contributions des variables à la création des axes.

```{r}
overshoot_pca$var$contrib
overshoot_pca$var$cos2
```

::: correction
6 variables contribuent presque également à la construction de l'axe 1
life_expectancy, hdi, per_capita_gdp, total_prod, total_cons, overshoot_day
L'axe 2 est formé essentiellement de biocapacité et nombre de pays requis (c'est le cas aussi de l'axe 4), l'axe 3 est la poplation.

L'information sur life_expectancy, hdi, per_capita_gdp,  total_cons, overshoot_day est bien représenté sur l'axe 1 (voir les cos2).

L'information concernant total_prod est répartie entre les axes 1,2, et 5

La bio capacity est surtout sur l'axe 2 et 4, tout comme le nombre de pays_requis


Pour résumer on peut dire :
Finalement  l'axe 1 est quasi parfaitement anti corrélé avec le jour du dépassement et fortement corrélé au nombre de terres, à la consommation totale. Cet axe nous parle donc bien de l'empreinte écologique. Il est aussi très corrélé positivement avec le hdi et life_expectancy. Ce qui signifie que plus le hdi est élevé plus la consommation totale par individu est importante, donc l'empreinte carbone est plus forte. Ca se confirme en regardant les contributions, les variables qui contribuent le plus sont  total_cons, et overshoot_day. L'anticorrélation entre overshoot_day et total cons est du meme ordre que dans l'exemple du decathlon avec course et saut : un pays au sein duquel les habitants consomment bcp est un pays avec un jour du dépassement faible, on épuise rapidement les ressources disponibles.

 l'axe 2 nous parle de la biocapacité on le voit sur le cercle.
Le nombre de pays nécessaires dépend de la production du pays et de sa consommation d'ou son positionnement entre les axes 1 et 2. (n'oublions pas qu'une partie n'est visble que dans le sdimensions suivantes) On en déduit que la biocapacité est indépendante de la consommation et de manière plus surprenante de la production... Souvent la production d'un pays n'est pas completement durable et donc n'est pas compté dans la biocapicité

Enfin la population est sur l'axe 3, cet axe est d'ailleurs essentiellement constitué par cette information, donc la population est indépendante de tout ça. la population d'un pays ne nous donne aucune information sur l'empreinte écologique d'un de ses habitants



Avec une ACP avec les poids on a des choses similaires sauf que le role de la population disparait
:::







### Observation des individus

```{r}
fviz_pca_ind(overshoot_pca,  col.ind = "cos2")
overshoot_pca$ind$contrib |> as.data.frame()|>
  rownames_to_column() |> as_tibble()|> arrange(-Dim.1)
overshoot_pca$ind$contrib |> as.data.frame()|>
  rownames_to_column() |> as_tibble()|> arrange(-Dim.2)

```

-   Que pensez vous du graphe des individus ? Quels sont les individus
    atypiques ? Quels sont les pays les mieux représentés ?


::: correction
Singapour, Qatar, Luxembourg United Arab Emirates, Canada et  Bahrain sont les plus gros
contributeurs de l'axe 1

Ce sont de très gros consommateurs et à part le Canade, ces pays ont des capcités de production durable plus faible que la moyenne.

Singapore est aussi un gros contributeurs de l'axe 2 car il produit vraiment beaucoup moins que la moyenne des pays (en terme de production durable)

Au contraire le Suriname et le Guyana, grace à la foret tropicale se caractérisent par une très grosse biocapacité.

Singapour est à la fois important pour l'axe 1 et 2, ca vaut la peine de refaire l'analyse sans singapour pour vérifier que toutes nos conclusions ne sont pas influencées par ce pays.


-   Comment l'ACP est-elle modifiée si on retire Singapour de l'analyse ?



### Impact de Singapour

```{r}

overshoot_pca2 <- PCA(overshoot_dta |> filter(!row.names(overshoot_dta) %in% c("Singapore")),
                      scale.unit = TRUE, quali.sup = c(4,5), quanti.sup = c(11))

fviz_eig(overshoot_pca2)
fviz_pca_var(overshoot_pca2)
fviz_pca_ind(overshoot_pca2,  col.ind = "cos2")

```

::: correction
Sans Singapour, rien ne change completement,On remarque le poids de Surinamn et Guuyana dans l'axe 2
:::

## MFA


De nombreuses variables nous parle de l'empreinte écologique, et les
variables nous parlant du développement du pays sont moins nombreuses.
On peut rééquilibrer l'importance des variables avec une méthodes vue en
cours, laquelle ?



::: correction
C'est l'AFM bien sur
AFM rééquilibre le poids de chacun des groupes de variables:\
:::


-   On souhaite identifier deux groupes de variables : les variables
    concernant l'empreinte carbone et les variables portant sur le
    développement du pays.



::: correction
Variables concernant les aspects écologiques: overshoot_day, biocapicity, total_prod, number_of_countries, total_cons
variables socio/eco life_expectancy, hdi, per_capita_gdi, pop
:::


-   Faites 2 ACP différentes sur chacun de ces deux groupes. Quelle est
    la première valeur propre dans les deux cas ?


```{r}
overshoot_pca_ecolo <- PCA(overshoot_dta ,
                      scale.unit = TRUE, quali.sup = c(4,5), quanti.sup = c(1,2,3,6, 11))

overshoot_pca_ecolo$eig[,1]

sd1 <- sqrt(overshoot_pca_ecolo$eig[1,1])
overshoot_pca_socio <- PCA(overshoot_dta,
                      scale.unit = TRUE, quali.sup = c(4,5), quanti.sup = c(7,8, 9, 10,  11,12))
overshoot_pca_socio$eig[,1]
sd2 <- sqrt(overshoot_pca_socio$eig[1,1])

```

::: correction
On a 5 variables dans le groupe des variables écologiques et 4 dans le groupe des variables/socio/demo/eco
Lorsqu'on résume chacune de ces variables à une seul axe on a pourtant à peu pres la meme information 2.8 pour l'un 2.6 pour l'autre.
Ce sont ces deux variances qui vont nous permettre de définir la métrique de l'AFM

:::

-   Rappelez en quoi la méthode mentionnée plus haut est une ACP
    particulière.

::: correction
  C'est une ACP dans laquelle le ppoids d'une variable est donné par l'inverse de la variance fois l'inverse de la première valeur propre qu'on vient de calculer
:::

-   On peut le faire directement, avec la fonction `̀MFA` de FactoMineR.
    Reprenez l'analyse précédente avec ce nouvel équilibre entre les
    différentes variables équilibrées. Quelles sont les informations que
    vous pouvez retenir ?


```{r}
overshoot_quanti <-  overshoot_dta |>
  select(where(is.numeric))  |> select(!number_of_earths_required)
```

```{r}

overshoot_mfa <- MFA(overshoot_quanti,
    group = c(4, 5), ## 4 variables dans le premier groupe, les 6 suivantes dans le groupe 2
    type = rep("s", 2) ## le groupe est de type quanti qu'on veut normaliser par l'ecart type, meme chose pour le groupe 2
)

plot(overshoot_mfa, choix = "var")
plot(overshoot_mfa, choix = "ind")
plot(overshoot_mfa, choix = "group")
```

# ACP pour compresser l'information : analyse d'images.

Cet exercice propose d'utiliser l'ACP pour compresser une image.

-   Connaissez-vous cette photo ?

![](img/abbey_road.jpg)

## Objectif

Une image en noir et blanc peut être envisagée comme une matrice où
chaque contient le niveau de gris du pixel qu'il représente. On stocke
donc autant de valeurs que de pixel. Peut on compresser l'information en utilisant l'ACP ?

## Préparation

-   Téléchargez cette image en suivant {le lien
    suivant](https://marieetienne.github.io/MAF/img/abbey_road.jpg) et
    placez la dans votre répertoire de travail dans
    `img/abbey_road.jpg`.

Nous allons utiliser les packages

```{r}
#| label: setup
#| echo: true
#| eval: true

library(imager) ## pour manipuler les images
library(parallel) # pour les calculs sur les images
```

```{r}
#| label: utils
#| echo: true
#| eval: true
#| message: false
#| code-fold: true

# Function to extract blocks in parallel, handling edge cases
complete_image_with_block_size <- function(channel_matrix, block_size) {
  h <- dim(channel_matrix)[1]  # Image height
  w <- dim(channel_matrix)[2]  # Image width

  h_complete <- ifelse( h %/% block_size == h/block_size, h, (h %/% block_size +1)*block_size)
  w_complete <- ifelse( w %/% block_size == w/block_size, w, (w %/% block_size +1)*block_size)
  channel_matrix_complete <- matrix(0, ncol= w_complete, nrow = h_complete)
  channel_matrix_complete[1:h,1:w] <- channel_matrix
  return(channel_matrix_complete)
}

image2block <- function(channel_matrix, block_size) {
  h <- dim(channel_matrix)[1]  # Image height
  w <- dim(channel_matrix)[2]  # Image width
  if(  h %/% block_size != h/block_size ){
    stop("height is not a multiple of block size")
  }
  if(  w %/% block_size != w/block_size ){
    stop("width is not a multiple of block size")
  }
  # Get the number of cores available for parallel processing
  num_cores <- parallel::detectCores() - 1  # Use one less than available cores to avoid system overload

  # Define indices for parallelization
  indices <- expand.grid(seq(1, h, by = block_size), seq(1, w, by = block_size))

  # Define the function to extract a block from the matrix, handling edge effects
  extract_single_block <- function(index) {
    i <- as.numeric(index[1])
    j <- as.numeric(index[2])

    print(i)

    # Handle cases where the block exceeds the image size (edge cases)
    block <- channel_matrix[i:min(i + block_size - 1, h), j:min(j + block_size - 1, w)]

    # Flatten the block into a single row
    # If the block is smaller than 4x4 (due to edges), pad with NA or 0 to maintain consistent size
    flattened_block <- as.vector(block)

    # Pad the flattened block with NA (or 0 if you prefer) to ensure each block is of size block_size^2
    padded_block <- rep(NA, block_size * block_size)
    padded_block[1:length(flattened_block)] <- flattened_block

    return(padded_block)
  }

  # Parallelize the block extraction using mclapply
  blocks <- mclapply(1:nrow(indices), function(k) extract_single_block(indices[k,]), mc.cores = num_cores)

  # Combine the results into a matrix
  blocks_matrix <- do.call(rbind, blocks)

  return(blocks_matrix)
}

block2image <- function(image_blocks, nrow_img, ncol_img, block_size) {
  n_col_block <- ncol_img/block_size
  n_row_block <- nrow_img/block_size

  X_reconstruction_list <- lapply(1:n_col_block, function(jblock){
    list_rows <- lapply(1:n_row_block, function(iblock){
      matrix( image_blocks[(jblock-1)*n_row_block + iblock,], ncol = block_size)
    })
    do.call(rbind, list_rows)
  })
  img <-do.call(cbind, X_reconstruction_list)
  return(img)
}
```

Pour bien comprendre comment sont décomposées les images en bloc de 4 par 4, on peut créer une matrice avec des cases numérotées et constater comment elle est transformée.

Un bloc de 4x4 pixels est un individu à 16 variables.

```{r micro_example}
test_image <- as.cimg(array(1:(20*16), dim= c(20, 16, 1, 1)), dim=c(32,16))
test_image[,]
plot(test_image, interpolate = FALSE )
test_image_blocks <- image2block(test_image[,], block_size = 4)
test_image_blocks
block2image(test_image_blocks, nrow(test_image), ncol(test_image), block_size = 4)

exemple2 <- matrix(10*rep(1:9, each=16), byrow= TRUE, ncol = 16)
exemple2_img <- block2image(exemple2, nrow_img = 12, ncol=12, block_size = 4)
plot(as.cimg(exemple2_img), interpolate = FALSE )
```

On encode donc l'image des Beattles avec ce système de patches. On choisit des patches de taille 4 x 4.

```{r}
#| message: false
#| label: patch
#| echo: true
#| eval: true
block_size <- 4
```


Pour  charger avec R, on utilise le package `imager` et la fonction `load.image`. puis la fonction `grayscale`pour bien s'assurer que l'image est codée en niveaux de gris.

```{r}
#| message: false
#| label: load_abbey
#|
image <- load.image('img/abbey_road.jpg')
image <- grayscale(image)
dim(image)
plot(as.cimg(image), interpolate = 0, rescale = FALSE,  axes = FALSE)

## on complete l'image pour qu'elle ait une dimension multiple de la taille des blocks
abbey_comp <- complete_image_with_block_size(image, block_size = block_size)
plot(as.cimg(abbey_comp), interpolate = 0, rescale = FALSE,  axes = FALSE)
abbey_matrix <- image2block(abbey_comp, block_size = block_size)
head(abbey_matrix)
```

-   Que représente le baycentre de ce nuage de points ?

::: correction
On fait la moyenne composante par composante du niveau de gris, on obtient un vecteur moyen de
${\mathbb{R}}^{16}$ que l'on peut réorganiser sous forme d'un patch 4 x
4. C'est le patch moyen.
:::


```{r}
#| label: barycentre
#| echo: true
#| eval: true
#| message: false

barycentre <- abbey_matrix |> as_data_frame() |> summarise_all(mean) |> as.numeric()
patch_moy <- barycentre |> matrix(ncol=block_size^2, nrow=1)

block2image(patch_moy, nrow_img = block_size, ncol_img = block_size, block_size = block_size)  |> as.cimg(width = block_size, col=block_size)  |> plot( interpolate = 0, axes = FALSE, rescale=FALSE)
##pour voir les différences on peut rescaler les niveaux de gris
block2image(patch_moy, nrow_img = block_size, ncol_img = block_size, block_size = block_size)  |> as.cimg(width = block_size, col=block_size)  |> plot( interpolate = 0, axes = FALSE, rescale=TRUE)
```

### ACP sur les patchs


-   Faire une ACP sur les données contenues dans `abbey_matrix` (Faut il faire une ACP normée ou non ? y a t il des variables supplémentaires)

Dans une ACP,  on travaille sur les variables centrées. On peut regarder l'image centrée

```{r}
X <- abbey_matrix |> sweep(2,barycentre, FUN = '-' )
```

on ne peut pas la dessiner car les nombres négatifs ne sont pas des niveaux de gris

-   Combien d'axes proposez vous de garder ?

```{r}
#| label: abbey_pca
#| echo: true
#| eval: true
#| message: false
abbey_pca <- PCA(abbey_matrix, scale.unit=FALSE, ncp=block_size*block_size, graph=FALSE)
abbey_pca$eig
```


- Que représente un vecteur propre ici ?

C'est un vecteur de $R^{16}$ on peut le mettre sous forme de patch


```{r}
abbey_pca$svd$V[,1] |> matrix(, ncol = 16) |> block2image(nrow_img = 4, ncol_img = 4, block_size = 4) |> as.cimg(width = 4, col=4)  |> plot( interpolate = 0, axes = FALSE, rescale=FALSE)

abbey_pca$svd$V[,1] |> matrix(, ncol = 16) |> block2image(nrow_img = 4, ncol_img = 4, block_size = 4) |> as.cimg(width = 4, col=4)  |> plot( interpolate = 0, axes = FALSE, rescale=TRUE)

abbey_pca$svd$V[,2] |> matrix(, ncol = 16) |> block2image(nrow_img = 4, ncol_img = 4, block_size = 4) |> as.cimg(width = 4, col=4)  |> plot( interpolate = 0, axes = FALSE, rescale=TRUE)

abbey_pca$svd$V[,3] |> matrix(, ncol = 16) |> block2image(nrow_img = 4, ncol_img = 4, block_size = 4) |> as.cimg(width = 4, col=4)  |> plot( interpolate = 0, axes = FALSE, rescale=TRUE)
```


### Reconstruction avec 1 composante

- on souhaite reconstruire l'image à partir de l'information contenue dans une seule composante. Expliquez à quoi correspond le code ci-dessous


```{r reconstruction1}
k <- 1
abbey_recons_centred <- abbey_pca$ind$coord[,1:k,drop=FALSE] %*% t(abbey_pca$svd$V)[1:k,,drop=FALSE]
#on rajoute la moyenne

# X_recon_centred <- block2image(abbey_recons_centred, nrow_img = nrow(abbey_comp), ncol_img = ncol(abbey_comp), block_size = 4)
# plot(as.cimg(array(X_recon_centred, dim=dim(abbey_comp))), interpolate = FALSE, rescale = FALSE)

abbey_recons <- abbey_recons_centred |> sweep(2, barycentre, FUN = "+")
## doit etre en 0 et 1
abbey_recons_norm <- pmin(abbey_recons,1)
X_recon <- block2image(abbey_recons_norm, nrow_img = nrow(abbey_comp), ncol_img = ncol(abbey_comp), block_size = block_size)
plot(as.cimg(array(X_recon, dim=dim(abbey_comp))), interpolate = FALSE, rescale = FALSE)
```


### Reconstruction avec 2 composantes

- Modifier le code pour garder 2 composantes principales (taille de l'image divisée par 8 au lieu de 16)

```{r reconstruction2}
k <- 2
abbey_recons_centred <- abbey_pca$ind$coord[,1:k,drop=FALSE] %*% t(abbey_pca$svd$V)[1:k,,drop=FALSE]

# X_recon_centred <- block2image(abbey_recons_centred, nrow_img = nrow(abbey_comp), ncol_img = ncol(abbey_comp), block_size = 4)
# plot(as.cimg(array(X_recon_centred, dim=dim(abbey_comp))), interpolate = FALSE, rescale = FALSE)

abbey_recons2 <- abbey_recons_centred |> sweep(2, barycentre, FUN = "+")
## un niveau de gris doit etre en 0 et 1
abbey_recons_norm <- pmax(pmin(abbey_recons2,1),0)

X_recon <- block2image(abbey_recons_norm, nrow_img = nrow(abbey_comp), ncol_img = ncol(abbey_comp), block_size = block_size)
plot(as.cimg(array(X_recon, dim=dim(abbey_comp))), interpolate = FALSE, rescale = FALSE)
```


### Evaluation de l'erreur de reconstruction

Une autre manière de faire serait de ne garder que la valeur moyenne pour chaque patch. On compare les deux méthodes ci-dessous.

- Ajoutez la comparaison avec la méthode qui utilise les 2 premiers composantes principales et commentez

```{r}
rmse_pca <- mean((abbey_recons-abbey_matrix )^2)
rmse_pca2 <- mean((abbey_recons2-abbey_matrix )^2)
#simple filter
abbey_filter <- matrix(rowMeans(abbey_matrix), ncol = 16, nrow = nrow(abbey_matrix))
rmse_filter <- mean((abbey_filter-abbey_matrix )^2)
```


### Exercice d'extension

- On peut aussi appliquer la méthode à une image couleur en traitant séparément les
    canaux R,G,B puis recombiner.
-  Quel intérêt y aurait-il à traiter simultanément les 3 couleurs plutot que séparément ?


**Remarque:** On a représenté l'image dans un espace de plus petite dimension en appliquant un filtre sur l'image de départ.
plutôt que de choisir un filtre strandard (la moyenne de tous les pixels d'un patch, on a appris le meilleur filtre, celui qui fait perdre le moins d'information).
En effet on a appris une transformation qui envoie une image de `r 712^2` pixels dans un espace de dimension `r round(712/4)^2`. Ce sont ces idées qui sont utilisées dans les réseaux de neurones convolutionnels à la base de l'IA pour les images. Si ce n'est qu'on travaille sur plusieurs tailles de filtre simultanément, sur un gros ensemble d'image et que le critère que l'on optimise n'est pas le même ... mais il y a des points communs.

-   Vous pouvez tester d'autres tailles de patches  (`block_size=8` ou `16`) et
    observer l'effet sur la variance expliquée et la qualité de
    reconstruction.
